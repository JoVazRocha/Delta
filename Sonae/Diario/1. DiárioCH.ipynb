{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do código Alteração\n",
    "\n",
    "### <u>Código que gera os ficheiros para estudo do comportamento da Delta</u>\n",
    "---\n",
    "O objectivo é receber dados da Delta e devolver um conjunto de métricas para prever roturas. Devolve um ficheiro que pode entrar no código 1 para juntar aos dados dos ninjas.\n",
    "\n",
    "---\n",
    "- Inputs\n",
    "\n",
    "> __Dados completos da Delta em pastas de ficheiros__ (de azul a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "                    ou\n",
    "\n",
    "> __Ficheiro já completo__ (de vermelho a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "- Outputs\n",
    "\n",
    "> __Ficheiro com produtos em causa__ em formato Long\n",
    "\n",
    "> __Métricas novas:__\n",
    "> - Roturas de Stock e Pré-rotura\n",
    "> - Sinal\n",
    "> - Ciclos e Adequação de Stock\n",
    "> - MSA (média de sellouts 10 dias antes)\n",
    "> - STK (Stock disponível + trânsito)\n",
    "> - (Novo) Balanço médio, mediano, liberal e conservador \n",
    "> - (Novo) Dias para a rotura de stock e de prateleira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import clickhouse_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:Blue\">Juntar Ficheiros</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar do Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX',\n",
    "                                       database = \"Delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 33.3 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Ler o ficheiro\n",
    "\n",
    "dfClick = client.query_df('SELECT * FROM Delta_dbd_120dias_I')\n",
    "\n",
    "dfClick = dfClick.sort_values(by = \"DATA\")\n",
    "dfClick = dfClick[dfClick.DATA != dfClick.DATA.unique()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2023-07-04 00:00:00', '2023-07-05 00:00:00', '2023-07-06 00:00:00',\n",
       " '2023-07-07 00:00:00', '2023-07-08 00:00:00', '2023-07-09 00:00:00',\n",
       " '2023-07-10 00:00:00', '2023-07-11 00:00:00', '2023-07-12 00:00:00',\n",
       " '2023-07-13 00:00:00',\n",
       " ...\n",
       " '2023-10-23 00:00:00', '2023-10-24 00:00:00', '2023-10-25 00:00:00',\n",
       " '2023-10-26 00:00:00', '2023-10-27 00:00:00', '2023-10-28 00:00:00',\n",
       " '2023-10-29 00:00:00', '2023-10-30 00:00:00', '2023-10-31 00:00:00',\n",
       " '2023-11-01 00:00:00']\n",
       "Length: 120, dtype: datetime64[s]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClick.DATA.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar ficheiros novos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfNovo = pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Diário\\\\31-10.xlsb\") #xlsx\n",
    "dfNovo = pd.read_excel(\"C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\Dados\\\\2.Delta\\\\Sonae\\\\Diario\\\\02-11.xlsb\")\n",
    "\n",
    "dfNovo = dfNovo[['DATA', 'EAN', 'DESC_ARTIGO', 'STORE', \"STORE_NAME\", \n",
    "                 \"SOH\", \"INTRANSIT\", \"EXPECTED\", \"PRES_STOCK\", \"VND (D-1)\", \"FLUXO\"]]\n",
    "\n",
    "# Renomear colunas e criar as do dia\n",
    "\n",
    "dfNovo = dfNovo.rename(columns={\"VND (D-1)\": \"SELLOUT_1_Dias_Antes\", \n",
    "                                \"SOH\": \"STOCK_1_Dias_Antes\",\n",
    "                                \"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "dfNovo[\"SELLOUT_1_Dias_Antes\"] = np.where(dfNovo[\"SELLOUT_1_Dias_Antes\"]<0, 0, dfNovo[\"SELLOUT_1_Dias_Antes\"])\n",
    "\n",
    "#DATETIME\n",
    "# se xlsx\n",
    "#dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], format='%Y-%m-%d') \n",
    "\n",
    "# se xlsb\n",
    "dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], unit='D', origin=datetime.datetime(1899, 12, 30)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDiario = pd.concat([dfClick, dfNovo], ignore_index=True, join='outer')\n",
    "\n",
    "dfFinal = dfDiario.copy()\n",
    "dfFinal['DATA'] = pd.to_datetime(dfFinal['DATA'], format='%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vendedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfVendedor=pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "dfVendedor=pd.read_excel(\"C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\Dados\\\\2.Delta\\\\Sonae\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "\n",
    "dfVendedor = dfVendedor.rename(columns={\"Cód. Loja\":\"STORE\"})\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=\"Vendedor\")\n",
    "\n",
    "# Criar coluna de reposição\n",
    "dfFinal = pd.merge(dfFinal, dfVendedor[[\"STORE\",\"Vendedor\"]], how=\"left\", on = \"STORE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['SELLOUT'] = dfFinal.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['STOCK'] = dfFinal.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['INTRANSIT'] = dfFinal.groupby([\"STORE\",\"EAN\"])['INTRANSIT_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['EXPECTED'] = dfFinal.groupby([\"STORE\",\"EAN\"])['EXPECTED_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['PRES_STOCK'] = dfFinal.groupby([\"STORE\",\"EAN\"])['PRES_STOCK_1_Dias_Antes'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>Ficheiro Lido<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas interessantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ROTURA\n",
    "> - PRÉ_ROTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir coluna de rotura (se stock menor ou igual a 0 e existe Linear)\n",
    "\n",
    "dfFinal[\"ROTURA\"] = np.where((dfFinal[\"STOCK\"] <= 0) & (dfFinal[\"PRES_STOCK\"] > 0), 1, 0)\n",
    "dfFinal[\"ROTURA_1_Dias_Antes\"] = np.where((dfFinal[\"STOCK_1_Dias_Antes\"] <= 0) & (dfFinal[\"PRES_STOCK_1_Dias_Antes\"] > 0), 1, 0)\n",
    "\n",
    "\n",
    "# Definir coluna de rotura (se stock menor ou igual a 0)\n",
    "\n",
    "dfFinal[\"PRE_ROTURA\"] = (dfFinal[\"STOCK\"] < dfFinal[\"PRES_STOCK\"]).astype(int)\n",
    "dfFinal[\"PRE_ROTURA_1_Dias_Antes\"] = (dfFinal[\"STOCK_1_Dias_Antes\"] < dfFinal[\"PRES_STOCK_1_Dias_Antes\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas 1, 4, 5 e 10 dias antes:\n",
    "\n",
    "- INSTRANSIT\n",
    "- EXPECTED\n",
    "- SELLOUT\n",
    "- CICLOS\n",
    "- Dias para Rotura\n",
    "- Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos dias antes:\n",
    "\n",
    "diasMet = [4, 5, 10]\n",
    "\n",
    "# Função para colunas de dias anteriores\n",
    "\n",
    "def dias(df, dia, coluna):         #dia é quantos dias antes\n",
    "    a=int(dia)\n",
    "\n",
    "    valores = df.groupby(['DESC_ARTIGO', 'STORE'])[coluna].transform(lambda x: x.shift(a))\n",
    "    valores[:a] = np.nan\n",
    "    \n",
    "    df.loc[:,f'{coluna}_{a}_Dias_Antes'] = valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STOCK\n",
    "> - SELLOUT\n",
    "> - INTRANSIT\n",
    "> - EXPECTED \n",
    "> - STK\n",
    "> - FORNECIMENTO\n",
    "> - CICLOS\n",
    "> - Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 37.2 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"STK\"] = dfFinal[\"STOCK\"] + dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "dfFinal[\"STK_1_Dias_Antes\"] = dfFinal[\"STOCK_1_Dias_Antes\"] + dfFinal[\"INTRANSIT_1_Dias_Antes\"] + dfFinal[\"EXPECTED_1_Dias_Antes\"]\n",
    "\n",
    "dfFinal[\"FORNECIMENTO\"] = dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "dfFinal[\"FORNECIMENTO_1_Dias_Antes\"] = dfFinal[\"INTRANSIT_1_Dias_Antes\"] + dfFinal[\"EXPECTED_1_Dias_Antes\"]\n",
    "\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"STOCK\"]/dfFinal[\"PRES_STOCK\"]\n",
    "dfFinal[\"CICLOS_1_Dias_Antes\"] = dfFinal[\"STOCK_1_Dias_Antes\"]/dfFinal[\"PRES_STOCK_1_Dias_Antes\"]\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"CICLOS\"].fillna(0)\n",
    "dfFinal.loc[np.isinf(dfFinal['CICLOS']), \"CICLOS\"] = 0\n",
    "\n",
    "dfFinal[\"Adequacao\"]= np.where(dfFinal[\"CICLOS\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]>=dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]<dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "dfFinal[\"Adequacao_1_Dias_Antes\"]= np.where(dfFinal[\"CICLOS_1_Dias_Antes\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS_1_Dias_Antes\"] <= 1.1) & (dfFinal[\"INTRANSIT_1_Dias_Antes\"]+dfFinal[\"EXPECTED_1_Dias_Antes\"]+dfFinal[\"STOCK_1_Dias_Antes\"]>=dfFinal[\"PRES_STOCK_1_Dias_Antes\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS_1_Dias_Antes\"] <= 1.1) & (dfFinal[\"INTRANSIT_1_Dias_Antes\"]+dfFinal[\"EXPECTED_1_Dias_Antes\"]+dfFinal[\"STOCK_1_Dias_Antes\"]<dfFinal[\"PRES_STOCK_1_Dias_Antes\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "\n",
    "\n",
    "for i in diasMet:   \n",
    "    dias(dfFinal, i, \"STOCK\")\n",
    "    dias(dfFinal, i, \"SELLOUT\")\n",
    "    dias(dfFinal, i, \"INTRANSIT\")\n",
    "    dias(dfFinal, i, \"EXPECTED\")\n",
    "    dias(dfFinal, i, \"STK\")\n",
    "    dias(dfFinal, i, \"FORNECIMENTO\")\n",
    "    dias(dfFinal, i, \"CICLOS\")\n",
    "    dias(dfFinal, i, \"Adequacao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MSA\n",
    "> - Balance: sellout / soma stock disponível mais transito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.1 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MSA do dia = média dos sellouts dos 10 dias anteriores ao dia em causa\n",
    "\n",
    "dfFinal[\"MSA10\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=10).mean())\n",
    "dfFinal[\"MSA10Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=10).std())\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA10\")\n",
    "    \n",
    "\n",
    "dfFinal[\"MSA20\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=20).mean())\n",
    "dfFinal[\"MSA20Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=20).std())\n",
    "  \n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA20\")\n",
    "\n",
    "\n",
    "# Balance do dia = razão entre o sellout médio e o stock para o dia actual\n",
    "dfFinal[\"Balance\"] =  dfFinal[\"MSA10\"] / dfFinal[\"STK\"]\n",
    "\n",
    "for i in diasMet:\n",
    "    \n",
    "    \n",
    "    valores = i * dfFinal[\"Balance\"].shift(i)\n",
    "    valores[:i] = np.nan\n",
    "    \n",
    "    dfFinal.loc[:,'%s_%s_Dias_Antes' % (\"Balance\", i)] = valores\n",
    "\n",
    "    \n",
    "\n",
    "dfFinal.loc[dfFinal.STK == 0, \"Balance\"] = 2\n",
    "dfFinal.Balance = dfFinal.Balance.fillna(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas 30, 60 e 120 dias antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "diasMetHist = [30, 60, 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Balance raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.2 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"Balance_Raw\"] =  dfFinal[\"SELLOUT\"] / dfFinal[\"STK\"]\n",
    "dfFinal.loc[dfFinal.STK == 0, \"Balance_Raw\"] = 2\n",
    "\n",
    "dfFinal[\"Balance_Raw_Count1\"] = np.where(dfFinal[\"Balance_Raw\"] < 0.5, 1, 0)\n",
    "\n",
    "dfFinal[\"Balance_Raw_Count2\"] = np.where(dfFinal[\"Balance_Raw\"] < 0.8, 1, 0)\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Balance_Raw_Count1_{i}\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Balance_Raw_Count1'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())\n",
    "    dfFinal[f\"Percentagem_Balance_Raw_Count2_{i}\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Balance_Raw_Count2'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Efeito fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.4 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#dfFinal['SELLOUT_fds'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([4,5,6])][\"SELLOUT\"].copy()\n",
    "#dfFinal['SELLOUT_semana'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([0,1,2,3])][\"SELLOUT\"].copy()\n",
    "dfFinal['SELLOUT_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"SELLOUT_1_Dias_Antes\"].copy()\n",
    "dfFinal['SELLOUT_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"SELLOUT_1_Dias_Antes\"].copy()\n",
    "\n",
    "dfFinal['SELLOUT_fds_Medio'] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "dfFinal['SELLOUT_semana_Medio'] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Efeito_Fds_{i}\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())/\n",
    "                                              (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())))-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Volatilidade\n",
    "> - Rotura \n",
    "> - Supply\n",
    "> - Percentagem de dias em Stock Borderline\n",
    "> - Percentagem de dias de Linear Incompleto\n",
    "> - Percentagem de dias sem vendas\n",
    "> - Tempo indisponível\n",
    "> - Vendas perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 14s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal=dfFinal.copy()\n",
    "# Sempre que é pedido abastecimento, fazer com que seja 1\n",
    "dfFinal[\"New_Supply\"] = np.where((dfFinal[\"EXPECTED_1_Dias_Antes\"]==0) & (dfFinal[\"EXPECTED\"]>0), 1, 0)\n",
    "dfFinal[\"Percentagem_Stock_Borderline_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<0.2*dfFinal[\"PRES_STOCK_1_Dias_Antes\"], 1, 0)\n",
    "dfFinal[\"Percentagem_Linear_Incompleto_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<dfFinal[\"PRES_STOCK_1_Dias_Antes\"], 1, 0)\n",
    "dfFinal[\"Sem_Vendas_1_Dias_Antes\"] = np.where(dfFinal[\"SELLOUT_1_Dias_Antes\"] == 0, 1, 0)\n",
    "dfFinal[\"Dias_Indisponivel_1_Dias_Antes\"] = np.where(dfFinal[\"ROTURA_1_Dias_Antes\"]==1, 1, 0)\n",
    "\n",
    "dfFinal['ROTURA_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"ROTURA_1_Dias_Antes\"].copy()\n",
    "dfFinal['ROTURA_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"ROTURA_1_Dias_Antes\"].copy()\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Volatilidade_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes']\\\n",
    "                                                .transform(lambda x: x.rolling(window=i, min_periods=1).std()) /\n",
    "                                                dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes']\\\n",
    "                                                .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Roturas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_1_Dias_Antes']\\\n",
    "                                           .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Roturas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_1_Dias_Antes']\\\n",
    "                                           .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Supply_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply']\\\n",
    "                                          .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Percentagem_Dias_Stock_Borderline_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Stock_Borderline_1_Dias_Antes']\\\n",
    "                                                         .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Percentagem_Dias_Linear_Incompleto_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Linear_Incompleto_1_Dias_Antes']\\\n",
    "                                                          .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Dias_Sem_Vendas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas_1_Dias_Antes']\\\n",
    "                                                   .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Dias_Indisponivel_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Dias_Indisponivel_1_Dias_Antes']\\\n",
    "                                                     .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Vendas_Perdidas_em_{i}_Dias\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum())\\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())) \\\n",
    "                                                  + \n",
    "                                               (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum()) \\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())))\n",
    "    \n",
    "dfFinal.Percentagem_Volatilidade_60 = dfFinal.Percentagem_Volatilidade_60.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo médio inter-supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.59 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal=dfFinal.copy()\n",
    "dfFinal[\"InterSupply\"] = np.where(dfFinal[\"EXPECTED\"]==0, 1, 0)\n",
    "\n",
    "groups = (dfFinal['InterSupply'] != dfFinal['InterSupply'].shift()).cumsum()\n",
    "result = dfFinal.groupby(groups).agg({'DATA': 'first', 'DESC_ARTIGO': 'first', 'STORE': 'first', 'InterSupply': 'sum'}).reset_index(drop=True)\n",
    "result = result[result['InterSupply'] > 0]\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=['InterSupply'])\n",
    "\n",
    "dfFinal = pd.merge(dfFinal, result, how=\"left\", on=[\"DATA\",\"DESC_ARTIGO\", \"STORE\"])\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_InterSupplyMed_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color=green> Função Contagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define a function to calculate weekday and weekend counts\n",
    "def contagem5dias(date):\n",
    "    weekday_count = 0\n",
    "    weekend_count = 0\n",
    "    \n",
    "    for _ in range(5):\n",
    "        if date.weekday() < 4:  # Monday to Thursday\n",
    "            weekday_count += 1\n",
    "        else:  # Friday to Sunday\n",
    "            weekend_count += 1\n",
    "        \n",
    "        date += timedelta(days=1)\n",
    "    \n",
    "    return pd.Series({\"CONTAGEM_SEMANA\": weekday_count, \"CONTAGEM_FIMSEMANA\": weekend_count})\n",
    "\n",
    "\n",
    "# Define a function to calculate weekday and weekend counts\n",
    "def contagem3dias(date):\n",
    "    weekday_count = 0\n",
    "    weekend_count = 0\n",
    "    \n",
    "    for _ in range(3):\n",
    "        if date.weekday() < 4:  # Monday to Thursday\n",
    "            weekday_count += 1\n",
    "        else:  # Friday to Sunday\n",
    "            weekend_count += 1\n",
    "        \n",
    "        date += timedelta(days=1)\n",
    "    \n",
    "    return pd.Series({\"CONTAGEM_SEMANA3\": weekday_count, \"CONTAGEM_FIMSEMANA3\": weekend_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Smart 3 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 54s\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "dfFinal[[\"CONTAGEM_SEMANA3\", \"CONTAGEM_FIMSEMANA3\"]] = dfFinal[\"DATA\"].apply(contagem3dias)\n",
    "dfFinal['CONTAGEM_FIMSEMANA3'] = dfFinal['CONTAGEM_FIMSEMANA3'].shift(-1)\n",
    "dfFinal['CONTAGEM_SEMANA3'] = dfFinal['CONTAGEM_SEMANA3'].shift(-1)\n",
    "\n",
    "dfFinal[\"Balance_Smart3\"] = ((\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA3'])\n",
    "    +\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA3']))\n",
    "     / dfFinal[\"STK\"])\n",
    "\n",
    "dfFinal[\"Balance_Smart3\"] = np.where(dfFinal[\"STK\"]<=0, -1, dfFinal[\"Balance_Smart3\"])\n",
    "dfFinal[\"Balance_Smart_3_Dias_Antes\"] = dfFinal[\"Balance_Smart3\"].shift(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Smart 5 dias antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 59s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "dfFinal[[\"CONTAGEM_SEMANA\", \"CONTAGEM_FIMSEMANA\"]] = dfFinal[\"DATA\"].apply(contagem5dias)\n",
    "dfFinal['CONTAGEM_FIMSEMANA'] = dfFinal['CONTAGEM_FIMSEMANA'].shift(-1)\n",
    "dfFinal['CONTAGEM_SEMANA'] = dfFinal['CONTAGEM_SEMANA'].shift(-1)\n",
    "\n",
    "\n",
    "dfFinal[\"Balance_Smart\"] = ((\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])\n",
    "    +\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA']))\n",
    "     / dfFinal[\"STK\"])\n",
    "\n",
    "dfFinal[\"Balance_Smart\"] = np.where(dfFinal[\"STK\"]<=0, -1, dfFinal[\"Balance_Smart\"])\n",
    "dfFinal[\"Balance_Smart_5_Dias_Antes\"] = dfFinal[\"Balance_Smart\"].shift(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = dfFinal.copy()\n",
    "# Dias para a rotura mas com o Sellout médio (móvel) dos últimos 10 dias \n",
    "dfFinal[\"Dias_para_Rotura_Stock\"] = dfFinal[\"STK_1_Dias_Antes\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_para_Rotura_Stock_5_Dias_Antes\"] = dfFinal[\"Dias_para_Rotura_Stock\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a métrica: Preslinear / med(Sellouts 10 dias)\n",
    "dfFinal['Dias_Duracao_Linear'] = dfFinal[\"PRES_STOCK\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_Duracao_Linear_5_Dias_Antes\"] = dfFinal[\"Dias_Duracao_Linear\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Desempenho\n",
    "> - Volatilidade de Sellout\n",
    "> - Sensibilidade\n",
    "> - PS Classificação\n",
    "> - Risco de Rotura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.14 s\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"Desempenho_de_Vendas_60\"] = np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>=0) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.151), \"Vendas com desempenho satisfatório\",\n",
    "                                     np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.151) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.317), \"Vendas com desempenho mediano\",\n",
    "                                     np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.317) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.433), \"Vendas com desempenho insatisfatório\",\n",
    "                                              \"Vendas com desempenho muito insatisfatório\")))\n",
    "\n",
    "\n",
    "                                     #np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.433) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=1), \n",
    "\n",
    "dfFinal[\"Volatilidade_SELLOUT\"] = np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 1.095, \" e com alta volatilidade de sellout.\",\n",
    "                                  np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 0.91 , \" e com média volatilidade de sellout.\",\n",
    "                                  np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 0.76 , \" e com baixa volatilidade de sellout.\",\n",
    "                                                                                           \" e com muito baixa volatilidade de sellout.\")))\n",
    "\n",
    "\n",
    "dfFinal[\"Sensibilidade_Rotura\"] = np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.33, \" Produto com muito elevada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.166666, \" Produto com elevada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.017, \" Produto com moderada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0, \" Produto com baixa propensão para rotura\",\n",
    "                                 \" Produto com muito baixa ou nula propensão para rotura\"))))\n",
    "\n",
    "dfFinal[\"Order_Sensibilidade_Rotura\"] = np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.33, 5,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.166666, 4,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.017, 3,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0, 2,\n",
    "                                 1))))\n",
    "\n",
    "dfFinal[\"PS_Classificacao_60\"] = np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.51) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==1) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.51) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==0) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.31) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==1) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS moderadamente excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] < 0.07) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]<0.8) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<0.07), \" e com PS escasso.\", \n",
    "                                                                                                                                                                                                            \" e com PS equilibrado.\"))))\n",
    "\n",
    "dfFinal[\"RISCO\"] = np.where((dfFinal.Balance_Smart > 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.2) & (dfFinal.Percentagem_Roturas_120 > 5), 1,\n",
    "                   np.where((dfFinal.Balance_Smart > 1) & (dfFinal.Balance_Smart < 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.6) & (dfFinal.Percentagem_Roturas_120 < 3), 2, 3))\n",
    "\n",
    "dfFinal[\"RISCO_ROTURA\"] = np.where((dfFinal.Balance_Smart >= 12.5) | (dfFinal.Balance_Smart < 0), \" O produto apresenta hoje muito elevada probabilidade de rotura a 5 dias caso não haja fornecimento.\", \n",
    "                          np.where((dfFinal.Balance_Smart >= 1.1) & (dfFinal.Balance_Smart < 12.5), \" O produto apresenta hoje elevada probabilidade de rotura a 5 dias caso não haja fornecimento.\",\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.833) & (dfFinal.Balance_Smart < 1.1), \" O produto apresenta hoje média probabilidade de rotura a 5 dias caso não haja fornecimento.\",\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.087) & (dfFinal.Balance_Smart < 0.833), \" O produto apresenta hoje baixa probabilidade de rotura a 5 dias caso não haja fornecimento.\", \n",
    "                                                                                                         \" O produto apresenta hoje muito baixa ou nula probabilidade de rotura a 5 dias caso não haja fornecimento.\"))))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Conclusao1\"] = dfFinal[\"Desempenho_de_Vendas_60\"] + dfFinal[\"Volatilidade_SELLOUT\"] \n",
    "dfFinal[\"Conclusao2\"] = dfFinal[\"Sensibilidade_Rotura\"] + dfFinal[\"PS_Classificacao_60\"] \n",
    "dfFinal[\"Conclusao3\"] = dfFinal[\"RISCO_ROTURA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ano Homólogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.8 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Ler o ficheiro\n",
    "\n",
    "df_2022 = client.query_df('SELECT * FROM Delta_2022')\n",
    "df_2022 = df_2022.sort_values(by = \"DATA\")\n",
    "dfFinal = pd.concat([df_2022,dfFinal])\n",
    "dfFinal.DATA = pd.to_datetime(dfFinal['DATA'], format='%Y-%m-%d') \n",
    "dfFinal = dfFinal.sort_values(by = [\"DATA\", \"STORE\", \"DESC_ARTIGO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = dfFinal.copy()\n",
    "# Extract year and month from the 'date' column\n",
    "dfFinal['Ano'] = dfFinal['DATA'].dt.year\n",
    "dfFinal['Ano_Anterior'] = dfFinal['Ano'] + 1\n",
    "dfFinal['Mes'] = dfFinal['DATA'].dt.month\n",
    "\n",
    "# Mês Actual\n",
    "media_mes_actual = dfFinal.groupby(['Ano', 'Mes', \"STORE\", \"EAN\"])['SELLOUT'].mean().reset_index()\n",
    "media_mes_actual.rename(columns={'SELLOUT': 'Media_Sellout_Mensal'}, inplace=True)\n",
    "\n",
    "# Mês Homólogo\n",
    "media_mes_homologo = dfFinal.groupby(['Ano_Anterior', 'Mes', \"STORE\", \"EAN\"])['SELLOUT'].mean().reset_index()\n",
    "media_mes_homologo.rename(columns={'SELLOUT': 'Media_Sellout_Mensal_Homologo'}, inplace=True)\n",
    "\n",
    "\n",
    "# Merge the monthly mean back into the original DataFrame\n",
    "dfFinal = dfFinal.merge(media_mes_actual, on=['Ano', 'Mes', \"STORE\", \"EAN\"], how=\"left\")\n",
    "dfFinal = dfFinal.merge(media_mes_homologo, left_on=['Ano', 'Mes', \"STORE\", \"EAN\"], right_on=['Ano_Anterior', 'Mes', \"STORE\", \"EAN\"], how=\"left\")\n",
    "\n",
    "dfFinal = dfFinal.drop(columns = [\"Ano_Anterior_x\", \"Ano_Anterior_y\"])\n",
    "\n",
    "# Calculate the monthly mean for the previous month\n",
    "media_mes_anterior = dfFinal.groupby(['Ano', 'Mes', \"STORE\", \"EAN\"])['SELLOUT'].mean().shift(1).reset_index()\n",
    "media_mes_anterior.rename(columns={'SELLOUT': 'Media_Sellout_Mensal_Mes_Anterior'}, inplace=True)\n",
    "\n",
    "dfFinal = dfFinal.merge(media_mes_anterior, on=['Ano', 'Mes', \"STORE\", \"EAN\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATA',\n",
       " 'DESC_ARTIGO',\n",
       " 'STORE_NAME',\n",
       " 'FLUXO',\n",
       " 'EAN',\n",
       " 'STORE',\n",
       " 'STOCK',\n",
       " 'SELLOUT',\n",
       " 'INTRANSIT',\n",
       " 'EXPECTED',\n",
       " 'PRES_STOCK',\n",
       " 'ROTURA',\n",
       " 'Adequacao',\n",
       " 'Adequacao_1_Dias_Antes',\n",
       " 'Adequacao_4_Dias_Antes',\n",
       " 'Adequacao_5_Dias_Antes',\n",
       " 'Adequacao_10_Dias_Antes',\n",
       " 'Desempenho_de_Vendas_60',\n",
       " 'Volatilidade_SELLOUT',\n",
       " 'Sensibilidade_Rotura',\n",
       " 'PS_Classificacao_60',\n",
       " 'RISCO_ROTURA',\n",
       " 'Conclusao1',\n",
       " 'Conclusao2',\n",
       " 'Conclusao3',\n",
       " 'Subinsignia',\n",
       " 'ROTURA_1_Dias_Antes',\n",
       " 'PRE_ROTURA',\n",
       " 'PRE_ROTURA_1_Dias_Antes',\n",
       " 'Balance_Raw_Count1',\n",
       " 'Balance_Raw_Count2',\n",
       " 'New_Supply',\n",
       " 'Percentagem_Stock_Borderline_1_Dias_Antes',\n",
       " 'Percentagem_Linear_Incompleto_1_Dias_Antes',\n",
       " 'Sem_Vendas_1_Dias_Antes',\n",
       " 'Dias_Indisponivel_1_Dias_Antes',\n",
       " 'Order_Sensibilidade_Rotura',\n",
       " 'RISCO',\n",
       " 'INTRANSIT_1_Dias_Antes',\n",
       " 'EXPECTED_1_Dias_Antes',\n",
       " 'PRES_STOCK_1_Dias_Antes',\n",
       " 'FORNECIMENTO_1_Dias_Antes',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'STOCK_1_Dias_Antes',\n",
       " 'SELLOUT_1_Dias_Antes',\n",
       " 'STK',\n",
       " 'STK_1_Dias_Antes',\n",
       " 'FORNECIMENTO',\n",
       " 'CICLOS',\n",
       " 'CICLOS_1_Dias_Antes',\n",
       " 'STOCK_4_Dias_Antes',\n",
       " 'SELLOUT_4_Dias_Antes',\n",
       " 'INTRANSIT_4_Dias_Antes',\n",
       " 'EXPECTED_4_Dias_Antes',\n",
       " 'STK_4_Dias_Antes',\n",
       " 'FORNECIMENTO_4_Dias_Antes',\n",
       " 'CICLOS_4_Dias_Antes',\n",
       " 'STOCK_5_Dias_Antes',\n",
       " 'SELLOUT_5_Dias_Antes',\n",
       " 'INTRANSIT_5_Dias_Antes',\n",
       " 'EXPECTED_5_Dias_Antes',\n",
       " 'STK_5_Dias_Antes',\n",
       " 'FORNECIMENTO_5_Dias_Antes',\n",
       " 'CICLOS_5_Dias_Antes',\n",
       " 'STOCK_10_Dias_Antes',\n",
       " 'SELLOUT_10_Dias_Antes',\n",
       " 'INTRANSIT_10_Dias_Antes',\n",
       " 'EXPECTED_10_Dias_Antes',\n",
       " 'STK_10_Dias_Antes',\n",
       " 'FORNECIMENTO_10_Dias_Antes',\n",
       " 'CICLOS_10_Dias_Antes',\n",
       " 'MSA10',\n",
       " 'MSA10Dp',\n",
       " 'MSA10_4_Dias_Antes',\n",
       " 'MSA10_5_Dias_Antes',\n",
       " 'MSA10_10_Dias_Antes',\n",
       " 'MSA20',\n",
       " 'MSA20Dp',\n",
       " 'MSA20_4_Dias_Antes',\n",
       " 'MSA20_5_Dias_Antes',\n",
       " 'MSA20_10_Dias_Antes',\n",
       " 'Balance',\n",
       " 'Balance_4_Dias_Antes',\n",
       " 'Balance_5_Dias_Antes',\n",
       " 'Balance_10_Dias_Antes',\n",
       " 'Balance_Raw',\n",
       " 'Percentagem_Balance_Raw_Count1_30',\n",
       " 'Percentagem_Balance_Raw_Count2_30',\n",
       " 'Percentagem_Balance_Raw_Count1_60',\n",
       " 'Percentagem_Balance_Raw_Count2_60',\n",
       " 'Percentagem_Balance_Raw_Count1_120',\n",
       " 'Percentagem_Balance_Raw_Count2_120',\n",
       " 'SELLOUT_fds_1_Dias_Antes',\n",
       " 'SELLOUT_semana_1_Dias_Antes',\n",
       " 'SELLOUT_fds_Medio',\n",
       " 'SELLOUT_semana_Medio',\n",
       " 'Percentagem_Efeito_Fds_30',\n",
       " 'Percentagem_Efeito_Fds_60',\n",
       " 'Percentagem_Efeito_Fds_120',\n",
       " 'ROTURA_fds_1_Dias_Antes',\n",
       " 'ROTURA_semana_1_Dias_Antes',\n",
       " 'Percentagem_Volatilidade_30',\n",
       " 'Percentagem_Roturas_30',\n",
       " 'Percentagem_Supply_30',\n",
       " 'Percentagem_Dias_Stock_Borderline_30',\n",
       " 'Percentagem_Dias_Linear_Incompleto_30',\n",
       " 'Percentagem_Dias_Sem_Vendas_30',\n",
       " 'Percentagem_Dias_Indisponivel_30',\n",
       " 'Vendas_Perdidas_em_30_Dias',\n",
       " 'Percentagem_Volatilidade_60',\n",
       " 'Percentagem_Roturas_60',\n",
       " 'Percentagem_Supply_60',\n",
       " 'Percentagem_Dias_Stock_Borderline_60',\n",
       " 'Percentagem_Dias_Linear_Incompleto_60',\n",
       " 'Percentagem_Dias_Sem_Vendas_60',\n",
       " 'Percentagem_Dias_Indisponivel_60',\n",
       " 'Vendas_Perdidas_em_60_Dias',\n",
       " 'Percentagem_Volatilidade_120',\n",
       " 'Percentagem_Roturas_120',\n",
       " 'Percentagem_Supply_120',\n",
       " 'Percentagem_Dias_Stock_Borderline_120',\n",
       " 'Percentagem_Dias_Linear_Incompleto_120',\n",
       " 'Percentagem_Dias_Sem_Vendas_120',\n",
       " 'Percentagem_Dias_Indisponivel_120',\n",
       " 'Vendas_Perdidas_em_120_Dias',\n",
       " 'Percentagem_InterSupplyMed_30',\n",
       " 'Percentagem_InterSupplyMed_60',\n",
       " 'Percentagem_InterSupplyMed_120',\n",
       " 'CONTAGEM_SEMANA3',\n",
       " 'CONTAGEM_FIMSEMANA3',\n",
       " 'Balance_Smart3',\n",
       " 'Balance_Smart_3_Dias_Antes',\n",
       " 'CONTAGEM_SEMANA',\n",
       " 'CONTAGEM_FIMSEMANA',\n",
       " 'Balance_Smart',\n",
       " 'Balance_Smart_5_Dias_Antes',\n",
       " 'Dias_para_Rotura_Stock',\n",
       " 'Dias_para_Rotura_Stock_5_Dias_Antes',\n",
       " 'Dias_Duracao_Linear',\n",
       " 'Dias_Duracao_Linear_5_Dias_Antes',\n",
       " 'Vendedor',\n",
       " 'InterSupply',\n",
       " 'Media_Sellout_Mensal',\n",
       " 'Media_Sellout_Mensal_Homologo',\n",
       " 'Media_Sellout_Mensal_Mes_Anterior']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfFinal = dfFinal.drop(columns=['Media_Sellout_Mensal_x',\n",
    "#                                 'Media_Sellout_Mensal_Homologo_x',\n",
    "#                                 'Media_Sellout_Mensal_Mes_Anterior_x',\n",
    "#                                 'Media_Sellout_Mensal_y',\n",
    "#                                 'Media_Sellout_Mensal_Homologo_y',\n",
    "#                                 'Media_Sellout_Mensal_Mes_Anterior_y'])\n",
    "dfFinal.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Escrever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dias certos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"ROTURA\"] = np.where(dfFinal[\"ROTURA\"]==1, \"ROTURA\", \"PRESENTE\")\n",
    "dfFinal['Subinsignia'] = dfFinal['STORE_NAME'].str.split().str[0]\n",
    "dfFinalEscrever = dfFinal.copy()\n",
    "\n",
    "dfEscrever120 = dfFinalEscrever[dfFinalEscrever.DATA.between(dfFinalEscrever.DATA.unique()[-121],dfFinalEscrever.DATA.unique()[-1])].copy()\n",
    "dfFinalEscrever = dfFinalEscrever.dropna(subset=['SELLOUT'])\n",
    "\n",
    "# Ficheiro Dia, Mês e 120 dias\n",
    "dfEscreverDia = dfFinalEscrever[dfFinalEscrever.DATA == dfFinalEscrever.DATA.unique()[-1]].copy()\n",
    "dfEscreverMes = dfFinalEscrever[dfFinalEscrever.DATA.between(dfFinalEscrever.DATA.unique()[-31],dfFinalEscrever.DATA.unique()[-1])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "from clickhouse_driver import Client\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX',\n",
    "                                       database = \"Delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A base Delta_dbd_Dia foi exportada para o clickhouse\n",
      "A base Delta_dbd_Mes foi exportada para o clickhouse\n",
      "A base Delta_dbd_120dias_P foi exportada para o clickhouse\n",
      "CPU times: total: 47.4 s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Estabelecer a base a ser lida\n",
    "\n",
    "bases = [1,2,3]\n",
    "\n",
    "for qualbase in bases:\n",
    "\n",
    "    if qualbase == 1:\n",
    "        baseCH = dfEscreverDia.copy()\n",
    "        tabela = \"Delta_dbd_Dia\" # BaseGoChillDia\n",
    "\n",
    "    elif qualbase == 2:\n",
    "        baseCH = dfEscreverMes.copy()\n",
    "        tabela = \"Delta_dbd_Mes\" # BaseGoChillMes\n",
    "\n",
    "    elif qualbase == 3:\n",
    "        baseCH = dfEscrever120.copy()\n",
    "        tabela = \"Delta_dbd_120dias_P\" # BaseDelta120\n",
    "\n",
    "    elif qualbase == 4:\n",
    "        baseCH = dfNinjas.copy()\n",
    "        tabela = \"Delta_dbd_Audit\" # BaseDeltaNinja\n",
    "\n",
    "    ## Algumas alterações\n",
    "    baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar a Data para datetime\n",
    "\n",
    "    baseCH.columns = [unidecode(col) for col in baseCH.columns]        # Tirar acentos e afins dos nomes das colunas porque \n",
    "                                                                       # o Clickhouse não gosta\n",
    "\n",
    "\n",
    "    ## Listas para definir os tipos de dados de cada coluna a inserit\n",
    "    data = [\"DATA\"]\n",
    "    texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object' or baseCH[col].dtype == 'string']\n",
    "    inteiros = [col for col in baseCH.columns if baseCH[col].dtype == 'int64' or baseCH[col].dtype == 'int32']\n",
    "    floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "    ## Mudar inteiros para floats porque senão não pode haver missing values\n",
    "    for col_name in inteiros:\n",
    "        baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "\n",
    "    ## Missing values em strings também estragam tudo\n",
    "    baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "    baseCH[texto] = baseCH[texto].astype(str)\n",
    "\n",
    "    #Função que vai fazer a schema\n",
    "    def schema(lista, tipo):\n",
    "        result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "        return result_list\n",
    "\n",
    "    # Schema a ser feito\n",
    "    data1 = schema(data, \"Date\")\n",
    "    texto1 = schema(texto, \"String\")\n",
    "    inteiros1 = schema(inteiros, \"Float64\")\n",
    "    floats1 = schema(floats, \"Float64\")\n",
    "    total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "    schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "    # Split the input string by commas\n",
    "    parts = schema.split(', ')\n",
    "    # Process each part and wrap the first word in double quotes\n",
    "    output_parts = []\n",
    "    for part in parts:\n",
    "        words = part.split()\n",
    "        if words:\n",
    "            first_word = words[0]\n",
    "            remaining_words = ' '.join(words[1:])\n",
    "            output_part = f'\"{first_word}\" {remaining_words}'\n",
    "            output_parts.append(output_part)\n",
    "    # Join the modified parts back into a string\n",
    "    schema = ', '.join(output_parts)\n",
    "\n",
    "    # Eliminar tabela que possa existir no CH com o mesmo nome\n",
    "    client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "    # Criar tabela no CH\n",
    "    client.command(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "            {schema}\n",
    "            ) ENGINE = MergeTree\n",
    "            ORDER BY (DATA)\n",
    "    ''')\n",
    "\n",
    "\n",
    "    # Exportar os dados para o clickhouse\n",
    "    client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())\n",
    "    print(f\"A base {tabela} foi exportada para o clickhouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>DESC_ARTIGO</th>\n",
       "      <th>STORE_NAME</th>\n",
       "      <th>FLUXO</th>\n",
       "      <th>ROTURA</th>\n",
       "      <th>Adequacao</th>\n",
       "      <th>Adequacao_1_Dias_Antes</th>\n",
       "      <th>Adequacao_4_Dias_Antes</th>\n",
       "      <th>Adequacao_5_Dias_Antes</th>\n",
       "      <th>Adequacao_10_Dias_Antes</th>\n",
       "      <th>...</th>\n",
       "      <th>Balance_Smart_5_Dias_Antes</th>\n",
       "      <th>Dias_para_Rotura_Stock</th>\n",
       "      <th>Dias_para_Rotura_Stock_5_Dias_Antes</th>\n",
       "      <th>Dias_Duracao_Linear</th>\n",
       "      <th>Dias_Duracao_Linear_5_Dias_Antes</th>\n",
       "      <th>Media_Sellout_Mensal</th>\n",
       "      <th>Media_Sellout_Mensal_Homologo</th>\n",
       "      <th>Media_Sellout_Mensal_Mes_Anterior</th>\n",
       "      <th>Vendedor</th>\n",
       "      <th>InterSupply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3606463</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>CHOCOLATE QUENTE CREMOSO CARAMELO 300G</td>\n",
       "      <td>MDL CARCAVELOS</td>\n",
       "      <td>PBL</td>\n",
       "      <td>PRESENTE</td>\n",
       "      <td></td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606464</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>CHOCOLATE QUENTE CREMOSO CARAMELO 300G</td>\n",
       "      <td>MDL PORTELA RALIS</td>\n",
       "      <td>PBL</td>\n",
       "      <td>PRESENTE</td>\n",
       "      <td></td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cláudia Loureiro</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606465</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>VB ADEGA MAYOR RES REG ALENT 75CL</td>\n",
       "      <td>CBD CAMPO MAIOR</td>\n",
       "      <td>DIRECTO</td>\n",
       "      <td>PRESENTE</td>\n",
       "      <td></td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606466</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>VT DIZERES ADEGA MAYOR ALENTEJO 75CL</td>\n",
       "      <td>CBD CAMPO MAIOR</td>\n",
       "      <td>DIRECTO</td>\n",
       "      <td>PRESENTE</td>\n",
       "      <td></td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.808081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606467</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>CERV C/ALC FRANZISKANER TP 35,5CL</td>\n",
       "      <td>MDL MADEIRASHOP.</td>\n",
       "      <td>DIRECTO</td>\n",
       "      <td>PRESENTE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Stock Insuf c Forn Adequado</td>\n",
       "      <td>Stock Insuf c Forn Adequado</td>\n",
       "      <td>Stock Insuf c Forn Adequado</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DATA                             DESC_ARTIGO         STORE_NAME  \\\n",
       "3606463 2023-11-02  CHOCOLATE QUENTE CREMOSO CARAMELO 300G     MDL CARCAVELOS   \n",
       "3606464 2023-11-02  CHOCOLATE QUENTE CREMOSO CARAMELO 300G  MDL PORTELA RALIS   \n",
       "3606465 2023-11-02       VB ADEGA MAYOR RES REG ALENT 75CL    CBD CAMPO MAIOR   \n",
       "3606466 2023-11-02    VT DIZERES ADEGA MAYOR ALENTEJO 75CL    CBD CAMPO MAIOR   \n",
       "3606467 2023-11-02       CERV C/ALC FRANZISKANER TP 35,5CL   MDL MADEIRASHOP.   \n",
       "\n",
       "           FLUXO    ROTURA Adequacao          Adequacao_1_Dias_Antes  \\\n",
       "3606463      PBL  PRESENTE            Stock Insuf c Forn Desadequado   \n",
       "3606464      PBL  PRESENTE            Stock Insuf c Forn Desadequado   \n",
       "3606465  DIRECTO  PRESENTE            Stock Insuf c Forn Desadequado   \n",
       "3606466  DIRECTO  PRESENTE                          Stock Suficiente   \n",
       "3606467  DIRECTO  PRESENTE                                             \n",
       "\n",
       "                 Adequacao_4_Dias_Antes          Adequacao_5_Dias_Antes  \\\n",
       "3606463                               -                               -   \n",
       "3606464                               -                               -   \n",
       "3606465  Stock Insuf c Forn Desadequado  Stock Insuf c Forn Desadequado   \n",
       "3606466                Stock Suficiente                Stock Suficiente   \n",
       "3606467     Stock Insuf c Forn Adequado     Stock Insuf c Forn Adequado   \n",
       "\n",
       "                Adequacao_10_Dias_Antes  ... Balance_Smart_5_Dias_Antes  \\\n",
       "3606463                               -  ...                        NaN   \n",
       "3606464                               -  ...                        NaN   \n",
       "3606465  Stock Insuf c Forn Desadequado  ...                        NaN   \n",
       "3606466                Stock Suficiente  ...                        NaN   \n",
       "3606467     Stock Insuf c Forn Adequado  ...                        NaN   \n",
       "\n",
       "        Dias_para_Rotura_Stock Dias_para_Rotura_Stock_5_Dias_Antes  \\\n",
       "3606463                    NaN                                 NaN   \n",
       "3606464                    NaN                                 NaN   \n",
       "3606465               2.222222                                 NaN   \n",
       "3606466              20.808081                                 NaN   \n",
       "3606467                    NaN                                 NaN   \n",
       "\n",
       "        Dias_Duracao_Linear Dias_Duracao_Linear_5_Dias_Antes  \\\n",
       "3606463                 NaN                              NaN   \n",
       "3606464                 NaN                              NaN   \n",
       "3606465                 NaN                              NaN   \n",
       "3606466                 NaN                              NaN   \n",
       "3606467                 NaN                              NaN   \n",
       "\n",
       "        Media_Sellout_Mensal Media_Sellout_Mensal_Homologo  \\\n",
       "3606463                  NaN                           NaN   \n",
       "3606464                  NaN                           NaN   \n",
       "3606465                  NaN                           NaN   \n",
       "3606466                  NaN                           NaN   \n",
       "3606467                  NaN                           NaN   \n",
       "\n",
       "        Media_Sellout_Mensal_Mes_Anterior          Vendedor  InterSupply  \n",
       "3606463                               NaN                 -          NaN  \n",
       "3606464                               NaN  Cláudia Loureiro          NaN  \n",
       "3606465                               NaN                 -          NaN  \n",
       "3606466                               NaN                 -          NaN  \n",
       "3606467                               NaN                 -          NaN  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseCH.tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passar para csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "escrever_csv(dfEscreverDia, \"DataBaseDeltaDia\")\n",
    "escrever_csv(dfEscreverMes, \"DataBaseDeltaMes\")\n",
    "escrever_csv(dfEscrever180, \"DataBaseDelta180Dias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ler ficheiro novo se estiver já no Clickhouse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Ler o ficheiro\n",
    "\n",
    "dfNovo = client.query_df('SELECT * FROM BaseDeltaSonae')\n",
    "\n",
    "dfNovo = dfNovo[['DATA', 'EAN', 'DESC_ARTIGO', 'STORE', \"STORE_NAME\", \n",
    "                         \"SOH\", \"INTRANSIT\", \"EXPECTED\", \"PRES_STOCK\", \"VND (D-1)\"]]\n",
    "\n",
    "# Renomear colunas e criar as do dia\n",
    "\n",
    "dfNovo = dfNovo.rename(columns={\"VND (D-1)\": \"SELLOUT_1_Dias_Antes\", \n",
    "                                \"SOH\": \"STOCK_1_Dias_Antes\",\n",
    "                                \"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "dfNovo[\"SELLOUT_1_Dias_Antes\"] = np.where(dfNovo[\"SELLOUT_1_Dias_Antes\"]<0, 0, dfNovo[\"SELLOUT_1_Dias_Antes\"])\n",
    "dfNovo['SELLOUT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['INTRANSIT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['INTRANSIT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['EXPECTED'] = dfNovo.groupby([\"STORE\",\"EAN\"])['EXPECTED_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['PRES_STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['PRES_STOCK_1_Dias_Antes'].shift(-1)\n",
    "\n",
    "#DATETIME\n",
    "# se xlsx\n",
    "#dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], format='%d-%m-%Y') \n",
    "\n",
    "# se xlsb\n",
    "dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], unit='d', origin=datetime.datetime(1899, 12, 30)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produtos específicos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Passar produtos e lojas para listas\n",
    "\n",
    "produtos = dfClick.DESC_ARTIGO.unique().tolist()\n",
    "lojas = dfClick.STORE.unique().tolist()\n",
    "\n",
    "# Alterar o dataframe para apenas incluir os produtos e lojas em causa\n",
    "\n",
    "dfNovo = dfNovo[(dfNovo[\"DESC_ARTIGO\"].isin(produtos)) & (dfNovo[\"STORE\"].isin(lojas))].copy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
