{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do código Alteração\n",
    "\n",
    "### <u>Código que gera os ficheiros para estudo do comportamento da Delta</u>\n",
    "---\n",
    "O objectivo é receber dados da Delta e devolver um conjunto de métricas para prever roturas. Devolve um ficheiro que pode entrar no código 1 para juntar aos dados dos ninjas.\n",
    "\n",
    "---\n",
    "- Inputs\n",
    "\n",
    "> __Dados completos da Delta em pastas de ficheiros__ (de azul a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "                    ou\n",
    "\n",
    "> __Ficheiro já completo__ (de vermelho a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "- Outputs\n",
    "\n",
    "> __Ficheiro com produtos em causa__ em formato Long\n",
    "\n",
    "> __Métricas novas:__\n",
    "> - Roturas de Stock e Pré-rotura\n",
    "> - Sinal\n",
    "> - Ciclos e Adequação de Stock\n",
    "> - MSA (média de sellouts 10 dias antes)\n",
    "> - STK (Stock disponível + trânsito)\n",
    "> - (Novo) Balanço médio, mediano, liberal e conservador \n",
    "> - (Novo) Dias para a rotura de stock e de prateleira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import clickhouse_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:Blue\">Juntar Ficheiros</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar do Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX',\n",
    "                                       database = \"Delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 34.8 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Ler o ficheiro\n",
    "\n",
    "dfClick = client.query_df('SELECT * FROM Delta_dbd_120dias')\n",
    "\n",
    "dfClick = dfClick.sort_values(by = \"DATA\")\n",
    "dfClick = dfClick[dfClick.DATA != dfClick.DATA.unique()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2023-07-02 00:00:00', '2023-07-03 00:00:00', '2023-07-04 00:00:00',\n",
       " '2023-07-05 00:00:00', '2023-07-06 00:00:00', '2023-07-07 00:00:00',\n",
       " '2023-07-08 00:00:00', '2023-07-09 00:00:00', '2023-07-10 00:00:00',\n",
       " '2023-07-11 00:00:00',\n",
       " ...\n",
       " '2023-10-21 00:00:00', '2023-10-22 00:00:00', '2023-10-23 00:00:00',\n",
       " '2023-10-24 00:00:00', '2023-10-25 00:00:00', '2023-10-26 00:00:00',\n",
       " '2023-10-27 00:00:00', '2023-10-28 00:00:00', '2023-10-29 00:00:00',\n",
       " '2023-10-30 00:00:00']\n",
       "Length: 120, dtype: datetime64[s]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClick.DATA.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar ficheiros novos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfNovo = pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Diário\\\\31-10.xlsb\") #xlsx\n",
    "dfNovo = pd.read_excel(\"C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\Dados\\\\2.Delta\\\\Sonae\\\\Diario\\\\31-10.xlsb\")\n",
    "\n",
    "dfNovo = dfNovo[['DATA', 'EAN', 'DESC_ARTIGO', 'STORE', \"STORE_NAME\", \n",
    "                 \"SOH\", \"INTRANSIT\", \"EXPECTED\", \"PRES_STOCK\", \"VND (D-1)\", \"FLUXO\"]]\n",
    "\n",
    "# Renomear colunas e criar as do dia\n",
    "\n",
    "dfNovo = dfNovo.rename(columns={\"VND (D-1)\": \"SELLOUT_1_Dias_Antes\", \n",
    "                                \"SOH\": \"STOCK_1_Dias_Antes\",\n",
    "                                \"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "dfNovo[\"SELLOUT_1_Dias_Antes\"] = np.where(dfNovo[\"SELLOUT_1_Dias_Antes\"]<0, 0, dfNovo[\"SELLOUT_1_Dias_Antes\"])\n",
    "\n",
    "#DATETIME\n",
    "# se xlsx\n",
    "#dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], format='%d-%m-%Y') \n",
    "\n",
    "# se xlsb\n",
    "dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], unit='D', origin=datetime.datetime(1899, 12, 30)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickhouse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Ler o ficheiro\n",
    "\n",
    "dfNovo = client.query_df('SELECT * FROM BaseDeltaSonae')\n",
    "\n",
    "dfNovo = dfNovo[['DATA', 'EAN', 'DESC_ARTIGO', 'STORE', \"STORE_NAME\", \n",
    "                         \"SOH\", \"INTRANSIT\", \"EXPECTED\", \"PRES_STOCK\", \"VND (D-1)\"]]\n",
    "\n",
    "# Renomear colunas e criar as do dia\n",
    "\n",
    "dfNovo = dfNovo.rename(columns={\"VND (D-1)\": \"SELLOUT_1_Dias_Antes\", \n",
    "                                \"SOH\": \"STOCK_1_Dias_Antes\",\n",
    "                                \"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "dfNovo[\"SELLOUT_1_Dias_Antes\"] = np.where(dfNovo[\"SELLOUT_1_Dias_Antes\"]<0, 0, dfNovo[\"SELLOUT_1_Dias_Antes\"])\n",
    "dfNovo['SELLOUT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['INTRANSIT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['INTRANSIT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['EXPECTED'] = dfNovo.groupby([\"STORE\",\"EAN\"])['EXPECTED_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['PRES_STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['PRES_STOCK_1_Dias_Antes'].shift(-1)\n",
    "\n",
    "#DATETIME\n",
    "# se xlsx\n",
    "#dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], format='%d-%m-%Y') \n",
    "\n",
    "# se xlsb\n",
    "dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], unit='d', origin=datetime.datetime(1899, 12, 30)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produtos específicos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Passar produtos e lojas para listas\n",
    "\n",
    "produtos = dfClick.DESC_ARTIGO.unique().tolist()\n",
    "lojas = dfClick.STORE.unique().tolist()\n",
    "\n",
    "# Alterar o dataframe para apenas incluir os produtos e lojas em causa\n",
    "\n",
    "dfNovo = dfNovo[(dfNovo[\"DESC_ARTIGO\"].isin(produtos)) & (dfNovo[\"STORE\"].isin(lojas))].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3604106 entries, 0 to 3604105\n",
      "Columns: 146 entries, DATA to Media_Sellout_Mensal_Mes_Anterior\n",
      "dtypes: float64(126), object(4), string(16)\n",
      "memory usage: 3.9+ GB\n"
     ]
    }
   ],
   "source": [
    "dfDiario = pd.concat([dfClick, dfNovo], ignore_index=True, join='outer')\n",
    "\n",
    "dfFinal = dfDiario.copy()\n",
    "dfFinal['DATA'] = pd.to_datetime(dfFinal['DATA'], format='%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vendedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfVendedor=pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "dfVendedor=pd.read_excel(\"C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\Dados\\\\2.Delta\\\\Sonae\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "\n",
    "dfVendedor = dfVendedor.rename(columns={\"Cód. Loja\":\"STORE\"})\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=\"Vendedor\")\n",
    "\n",
    "# Criar coluna de reposição\n",
    "dfFinal = pd.merge(dfFinal, dfVendedor[[\"STORE\",\"Vendedor\"]], how=\"left\", on = \"STORE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['SELLOUT'] = dfFinal.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['STOCK'] = dfFinal.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['INTRANSIT'] = dfFinal.groupby([\"STORE\",\"EAN\"])['INTRANSIT_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['EXPECTED'] = dfFinal.groupby([\"STORE\",\"EAN\"])['EXPECTED_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['PRES_STOCK'] = dfFinal.groupby([\"STORE\",\"EAN\"])['PRES_STOCK_1_Dias_Antes'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>Ficheiro Lido<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas interessantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ROTURA\n",
    "> - PRÉ_ROTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir coluna de rotura (se stock menor ou igual a 0 e existe Linear)\n",
    "\n",
    "dfFinal[\"ROTURA\"] = np.where((dfFinal[\"STOCK\"] <= 0) & (dfFinal[\"PRES_STOCK\"] > 0), 1, 0)\n",
    "dfFinal[\"ROTURA_1_Dias_Antes\"] = np.where((dfFinal[\"STOCK_1_Dias_Antes\"] <= 0) & (dfFinal[\"PRES_STOCK_1_Dias_Antes\"] > 0), 1, 0)\n",
    "\n",
    "\n",
    "# Definir coluna de rotura (se stock menor ou igual a 0)\n",
    "\n",
    "dfFinal[\"PRE_ROTURA\"] = (dfFinal[\"STOCK\"] < dfFinal[\"PRES_STOCK\"]).astype(int)\n",
    "dfFinal[\"PRE_ROTURA_1_Dias_Antes\"] = (dfFinal[\"STOCK_1_Dias_Antes\"] < dfFinal[\"PRES_STOCK_1_Dias_Antes\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas 1, 4, 5 e 10 dias antes:\n",
    "\n",
    "- INSTRANSIT\n",
    "- EXPECTED\n",
    "- SELLOUT\n",
    "- CICLOS\n",
    "- Dias para Rotura\n",
    "- Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos dias antes:\n",
    "\n",
    "diasMet = [4, 5, 10]\n",
    "\n",
    "# Função para colunas de dias anteriores\n",
    "\n",
    "def dias(df, dia, coluna):         #dia é quantos dias antes\n",
    "    a=int(dia)\n",
    "\n",
    "    valores = df.groupby(['DESC_ARTIGO', 'STORE'])[coluna].transform(lambda x: x.shift(a))\n",
    "    valores[:a] = np.nan\n",
    "    \n",
    "    df.loc[:,f'{coluna}_{a}_Dias_Antes'] = valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STOCK\n",
    "> - SELLOUT\n",
    "> - INTRANSIT\n",
    "> - EXPECTED \n",
    "> - STK\n",
    "> - FORNECIMENTO\n",
    "> - CICLOS\n",
    "> - Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.3 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"STK\"] = dfFinal[\"STOCK\"] + dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "dfFinal[\"STK_1_Dias_Antes\"] = dfFinal[\"STOCK_1_Dias_Antes\"] + dfFinal[\"INTRANSIT_1_Dias_Antes\"] + dfFinal[\"EXPECTED_1_Dias_Antes\"]\n",
    "\n",
    "dfFinal[\"FORNECIMENTO\"] = dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "dfFinal[\"FORNECIMENTO_1_Dias_Antes\"] = dfFinal[\"INTRANSIT_1_Dias_Antes\"] + dfFinal[\"EXPECTED_1_Dias_Antes\"]\n",
    "\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"STOCK\"]/dfFinal[\"PRES_STOCK\"]\n",
    "dfFinal[\"CICLOS_1_Dias_Antes\"] = dfFinal[\"STOCK_1_Dias_Antes\"]/dfFinal[\"PRES_STOCK_1_Dias_Antes\"]\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"CICLOS\"].fillna(0)\n",
    "dfFinal.loc[np.isinf(dfFinal['CICLOS']), \"CICLOS\"] = 0\n",
    "\n",
    "dfFinal[\"Adequacao\"]= np.where(dfFinal[\"CICLOS\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]>=dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]<dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "dfFinal[\"Adequacao_1_Dias_Antes\"]= np.where(dfFinal[\"CICLOS_1_Dias_Antes\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS_1_Dias_Antes\"] <= 1.1) & (dfFinal[\"INTRANSIT_1_Dias_Antes\"]+dfFinal[\"EXPECTED_1_Dias_Antes\"]+dfFinal[\"STOCK_1_Dias_Antes\"]>=dfFinal[\"PRES_STOCK_1_Dias_Antes\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS_1_Dias_Antes\"] <= 1.1) & (dfFinal[\"INTRANSIT_1_Dias_Antes\"]+dfFinal[\"EXPECTED_1_Dias_Antes\"]+dfFinal[\"STOCK_1_Dias_Antes\"]<dfFinal[\"PRES_STOCK_1_Dias_Antes\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "\n",
    "\n",
    "for i in diasMet:   \n",
    "    dias(dfFinal, i, \"STOCK\")\n",
    "    dias(dfFinal, i, \"SELLOUT\")\n",
    "    dias(dfFinal, i, \"INTRANSIT\")\n",
    "    dias(dfFinal, i, \"EXPECTED\")\n",
    "    dias(dfFinal, i, \"STK\")\n",
    "    dias(dfFinal, i, \"FORNECIMENTO\")\n",
    "    dias(dfFinal, i, \"CICLOS\")\n",
    "    dias(dfFinal, i, \"Adequacao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MSA\n",
    "> - Balance: sellout / soma stock disponível mais transito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.7 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MSA do dia = média dos sellouts dos 10 dias anteriores ao dia em causa\n",
    "\n",
    "dfFinal[\"MSA10\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=10).mean())\n",
    "dfFinal[\"MSA10Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=10).std())\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA10\")\n",
    "    \n",
    "\n",
    "dfFinal[\"MSA20\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=20).mean())\n",
    "dfFinal[\"MSA20Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=20).std())\n",
    "  \n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA20\")\n",
    "\n",
    "\n",
    "# Balance do dia = razão entre o sellout médio e o stock para o dia actual\n",
    "dfFinal[\"Balance\"] =  dfFinal[\"MSA10\"] / dfFinal[\"STK\"]\n",
    "\n",
    "for i in diasMet:\n",
    "    \n",
    "    \n",
    "    valores = i * dfFinal[\"Balance\"].shift(i)\n",
    "    valores[:i] = np.nan\n",
    "    \n",
    "    dfFinal.loc[:,'%s_%s_Dias_Antes' % (\"Balance\", i)] = valores\n",
    "\n",
    "    \n",
    "\n",
    "dfFinal.loc[dfFinal.STK == 0, \"Balance\"] = 2\n",
    "dfFinal.Balance = dfFinal.Balance.fillna(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas 30, 60 e 120 dias antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "diasMetHist = [30, 60, 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Balance raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.94 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"Balance_Raw\"] =  dfFinal[\"SELLOUT\"] / dfFinal[\"STK\"]\n",
    "dfFinal.loc[dfFinal.STK == 0, \"Balance_Raw\"] = 2\n",
    "\n",
    "dfFinal[\"Balance_Raw_Count1\"] = np.where(dfFinal[\"Balance_Raw\"] < 0.5, 1, 0)\n",
    "\n",
    "dfFinal[\"Balance_Raw_Count2\"] = np.where(dfFinal[\"Balance_Raw\"] < 0.8, 1, 0)\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Balance_Raw_Count1_{i}\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Balance_Raw_Count1'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())\n",
    "    dfFinal[f\"Percentagem_Balance_Raw_Count2_{i}\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Balance_Raw_Count2'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Efeito fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.6 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#dfFinal['SELLOUT_fds'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([4,5,6])][\"SELLOUT\"].copy()\n",
    "#dfFinal['SELLOUT_semana'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([0,1,2,3])][\"SELLOUT\"].copy()\n",
    "dfFinal['SELLOUT_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"SELLOUT_1_Dias_Antes\"].copy()\n",
    "dfFinal['SELLOUT_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"SELLOUT_1_Dias_Antes\"].copy()\n",
    "\n",
    "dfFinal['SELLOUT_fds_Medio'] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "dfFinal['SELLOUT_semana_Medio'] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Efeito_Fds_{i}\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())/\n",
    "                                              (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())))-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Volatilidade\n",
    "> - Rotura \n",
    "> - Supply\n",
    "> - Percentagem de dias em Stock Borderline\n",
    "> - Percentagem de dias de Linear Incompleto\n",
    "> - Percentagem de dias sem vendas\n",
    "> - Tempo indisponível\n",
    "> - Vendas perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 3s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal=dfFinal.copy()\n",
    "# Sempre que é pedido abastecimento, fazer com que seja 1\n",
    "dfFinal[\"New_Supply\"] = np.where((dfFinal[\"EXPECTED_1_Dias_Antes\"]==0) & (dfFinal[\"EXPECTED\"]>0), 1, 0)\n",
    "dfFinal[\"Percentagem_Stock_Borderline_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<0.2*dfFinal[\"PRES_STOCK_1_Dias_Antes\"], 1, 0)\n",
    "dfFinal[\"Percentagem_Linear_Incompleto_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<dfFinal[\"PRES_STOCK_1_Dias_Antes\"], 1, 0)\n",
    "dfFinal[\"Sem_Vendas_1_Dias_Antes\"] = np.where(dfFinal[\"SELLOUT_1_Dias_Antes\"] == 0, 1, 0)\n",
    "dfFinal[\"Dias_Indisponivel_1_Dias_Antes\"] = np.where(dfFinal[\"ROTURA_1_Dias_Antes\"]==1, 1, 0)\n",
    "\n",
    "dfFinal['ROTURA_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"ROTURA_1_Dias_Antes\"].copy()\n",
    "dfFinal['ROTURA_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"ROTURA_1_Dias_Antes\"].copy()\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Volatilidade_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes']\\\n",
    "                                                .transform(lambda x: x.rolling(window=i, min_periods=1).std()) /\n",
    "                                                dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes']\\\n",
    "                                                .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Roturas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_1_Dias_Antes']\\\n",
    "                                           .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Roturas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_1_Dias_Antes']\\\n",
    "                                           .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Supply_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply']\\\n",
    "                                          .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Percentagem_Dias_Stock_Borderline_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Stock_Borderline_1_Dias_Antes']\\\n",
    "                                                         .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Percentagem_Dias_Linear_Incompleto_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Linear_Incompleto_1_Dias_Antes']\\\n",
    "                                                          .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Dias_Sem_Vendas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas_1_Dias_Antes']\\\n",
    "                                                   .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "    \n",
    "    dfFinal[f\"Percentagem_Dias_Indisponivel_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Dias_Indisponivel_1_Dias_Antes']\\\n",
    "                                                     .transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n",
    "\n",
    "    dfFinal[f\"Vendas_Perdidas_em_{i}_Dias\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum())\\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())) \\\n",
    "                                                  + \n",
    "                                               (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum()) \\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean())))\n",
    "    \n",
    "dfFinal.Percentagem_Volatilidade_60 = dfFinal.Percentagem_Volatilidade_60.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo médio inter-supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.45 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal=dfFinal.copy()\n",
    "dfFinal[\"InterSupply\"] = np.where(dfFinal[\"EXPECTED\"]==0, 1, 0)\n",
    "\n",
    "groups = (dfFinal['InterSupply'] != dfFinal['InterSupply'].shift()).cumsum()\n",
    "result = dfFinal.groupby(groups).agg({'DATA': 'first', 'DESC_ARTIGO': 'first', 'STORE': 'first', 'InterSupply': 'sum'}).reset_index(drop=True)\n",
    "result = result[result['InterSupply'] > 0]\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=['InterSupply'])\n",
    "\n",
    "dfFinal = pd.merge(dfFinal, result, how=\"left\", on=[\"DATA\",\"DESC_ARTIGO\", \"STORE\"])\n",
    "\n",
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_InterSupplyMed_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color=green> Função Contagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define a function to calculate weekday and weekend counts\n",
    "def contagem5dias(date):\n",
    "    weekday_count = 0\n",
    "    weekend_count = 0\n",
    "    \n",
    "    for _ in range(5):\n",
    "        if date.weekday() < 4:  # Monday to Thursday\n",
    "            weekday_count += 1\n",
    "        else:  # Friday to Sunday\n",
    "            weekend_count += 1\n",
    "        \n",
    "        date += timedelta(days=1)\n",
    "    \n",
    "    return pd.Series({\"CONTAGEM_SEMANA\": weekday_count, \"CONTAGEM_FIMSEMANA\": weekend_count})\n",
    "\n",
    "\n",
    "# Define a function to calculate weekday and weekend counts\n",
    "def contagem3dias(date):\n",
    "    weekday_count = 0\n",
    "    weekend_count = 0\n",
    "    \n",
    "    for _ in range(3):\n",
    "        if date.weekday() < 4:  # Monday to Thursday\n",
    "            weekday_count += 1\n",
    "        else:  # Friday to Sunday\n",
    "            weekend_count += 1\n",
    "        \n",
    "        date += timedelta(days=1)\n",
    "    \n",
    "    return pd.Series({\"CONTAGEM_SEMANA3\": weekday_count, \"CONTAGEM_FIMSEMANA3\": weekend_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Smart 3 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 38s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "dfFinal[[\"CONTAGEM_SEMANA3\", \"CONTAGEM_FIMSEMANA3\"]] = dfFinal[\"DATA\"].apply(contagem3dias)\n",
    "dfFinal['CONTAGEM_FIMSEMANA3'] = dfFinal['CONTAGEM_FIMSEMANA3'].shift(-1)\n",
    "dfFinal['CONTAGEM_SEMANA3'] = dfFinal['CONTAGEM_SEMANA3'].shift(-1)\n",
    "\n",
    "dfFinal[\"Balance_Smart3\"] = ((\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA3'])\n",
    "    +\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA3']))\n",
    "     / dfFinal[\"STK\"])\n",
    "\n",
    "dfFinal[\"Balance_Smart3\"] = np.where(dfFinal[\"STK\"]<=0, -1, dfFinal[\"Balance_Smart3\"])\n",
    "dfFinal[\"Balance_Smart_3_Dias_Antes\"] = dfFinal[\"Balance_Smart3\"].shift(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Smart 5 dias antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 3s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "dfFinal[[\"CONTAGEM_SEMANA\", \"CONTAGEM_FIMSEMANA\"]] = dfFinal[\"DATA\"].apply(contagem5dias)\n",
    "dfFinal['CONTAGEM_FIMSEMANA'] = dfFinal['CONTAGEM_FIMSEMANA'].shift(-1)\n",
    "dfFinal['CONTAGEM_SEMANA'] = dfFinal['CONTAGEM_SEMANA'].shift(-1)\n",
    "\n",
    "\n",
    "dfFinal[\"Balance_Smart\"] = ((\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])\n",
    "    +\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA']))\n",
    "     / dfFinal[\"STK\"])\n",
    "\n",
    "dfFinal[\"Balance_Smart\"] = np.where(dfFinal[\"STK\"]<=0, -1, dfFinal[\"Balance_Smart\"])\n",
    "dfFinal[\"Balance_Smart_5_Dias_Antes\"] = dfFinal[\"Balance_Smart\"].shift(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = dfFinal.copy()\n",
    "# Dias para a rotura mas com o Sellout médio (móvel) dos últimos 10 dias \n",
    "dfFinal[\"Dias_para_Rotura_Stock\"] = dfFinal[\"STK_1_Dias_Antes\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_para_Rotura_Stock_5_Dias_Antes\"] = dfFinal[\"Dias_para_Rotura_Stock\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a métrica: Preslinear / med(Sellouts 10 dias)\n",
    "dfFinal['Dias_Duracao_Linear'] = dfFinal[\"PRES_STOCK\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_Duracao_Linear_5_Dias_Antes\"] = dfFinal[\"Dias_Duracao_Linear\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Desempenho\n",
    "> - Volatilidade de Sellout\n",
    "> - Sensibilidade\n",
    "> - PS Classificação\n",
    "> - Risco de Rotura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.5 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"Desempenho_de_Vendas_60\"] = np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>=0) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.151), \"Vendas com desempenho satisfatório\",\n",
    "                                     np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.151) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.317), \"Vendas com desempenho mediano\",\n",
    "                                     np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.317) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=0.433), \"Vendas com desempenho insatisfatório\",\n",
    "                                              \"Vendas com desempenho muito insatisfatório\")))\n",
    "\n",
    "\n",
    "                                     #np.where((dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.433) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<=1), \n",
    "\n",
    "dfFinal[\"Volatilidade_SELLOUT\"] = np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 1.095, \" e com alta volatilidade de sellout.\",\n",
    "                                  np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 0.91 , \" e com média volatilidade de sellout.\",\n",
    "                                  np.where(dfFinal[\"Percentagem_Volatilidade_60\"] > 0.76 , \" e com baixa volatilidade de sellout.\",\n",
    "                                                                                           \" e com muito baixa volatilidade de sellout.\")))\n",
    "\n",
    "\n",
    "dfFinal[\"Sensibilidade_Rotura\"] = np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.33, \" Produto com muito elevada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.166666, \" Produto com elevada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.017, \" Produto com moderada propensão para rotura\",\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0, \" Produto com baixa propensão para rotura\",\n",
    "                                 \" Produto com muito baixa ou nula propensão para rotura\"))))\n",
    "\n",
    "dfFinal[\"Order_Sensibilidade_Rotura\"] = np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.33, 5,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.166666, 4,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0.017, 3,\n",
    "                                 np.where(dfFinal[\"Percentagem_Roturas_60\"]>0, 2,\n",
    "                                 1))))\n",
    "\n",
    "dfFinal[\"PS_Classificacao_60\"] = np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.51) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==1) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.51) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==0) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] > 0.31) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]==1) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]>0.29), \" e com PS moderadamente excessivo.\",\n",
    "                                 np.where((dfFinal[\"Percentagem_Dias_Linear_Incompleto_60\"] < 0.07) & (dfFinal[\"Percentagem_Balance_Raw_Count1_60\"]<0.8) & (dfFinal[\"Percentagem_Dias_Sem_Vendas_60\"]<0.07), \" e com PS escasso.\", \n",
    "                                                                                                                                                                                                            \" e com PS equilibrado.\"))))\n",
    "\n",
    "dfFinal[\"RISCO\"] = np.where((dfFinal.Balance_Smart > 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.2) & (dfFinal.Percentagem_Roturas_120 > 5), 1,\n",
    "                   np.where((dfFinal.Balance_Smart > 1) & (dfFinal.Balance_Smart < 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.6) & (dfFinal.Percentagem_Roturas_120 < 3), 2, 3))\n",
    "\n",
    "dfFinal[\"RISCO_ROTURA\"] = np.where((dfFinal.Balance_Smart >= 12.5) | (dfFinal.Balance_Smart < 0), \" O produto apresenta hoje muito elevada probabilidade de rotura a 5 dias caso não haja fornecimento.\", \n",
    "                          np.where((dfFinal.Balance_Smart >= 1.1) & (dfFinal.Balance_Smart < 12.5), \" O produto apresenta hoje elevada probabilidade de rotura a 5 dias caso não haja fornecimento.\",\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.833) & (dfFinal.Balance_Smart < 1.1), \" O produto apresenta hoje média probabilidade de rotura a 5 dias caso não haja fornecimento.\",\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.087) & (dfFinal.Balance_Smart < 0.833), \" O produto apresenta hoje baixa probabilidade de rotura a 5 dias caso não haja fornecimento.\", \n",
    "                                                                                                         \" O produto apresenta hoje muito baixa ou nula probabilidade de rotura a 5 dias caso não haja fornecimento.\"))))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Conclusao1\"] = dfFinal[\"Desempenho_de_Vendas_60\"] + dfFinal[\"Volatilidade_SELLOUT\"] \n",
    "dfFinal[\"Conclusao2\"] = dfFinal[\"Sensibilidade_Rotura\"] + dfFinal[\"PS_Classificacao_60\"] \n",
    "dfFinal[\"Conclusao3\"] = dfFinal[\"RISCO_ROTURA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Escrever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dias certos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"ROTURA\"] = np.where(dfFinal[\"ROTURA\"]==1, \"ROTURA\", \"PRESENTE\")\n",
    "dfFinal['Subinsignia'] = dfFinal['STORE_NAME'].str.split().str[0]\n",
    "dfFinalEscrever = dfFinal.copy()\n",
    "\n",
    "dfEscrever120 = dfFinalEscrever[dfFinalEscrever.DATA.between(dfFinalEscrever.DATA.unique()[-121],dfFinalEscrever.DATA.unique()[-1])].copy()\n",
    "dfFinalEscrever = dfFinalEscrever.dropna(subset=['SELLOUT'])\n",
    "\n",
    "# Ficheiro Dia, Mês e 120 dias\n",
    "dfEscreverDia = dfFinalEscrever[dfFinalEscrever.DATA == dfFinalEscrever.DATA.unique()[-1]].copy()\n",
    "dfEscreverMes = dfFinalEscrever[dfFinalEscrever.DATA.between(dfFinalEscrever.DATA.unique()[-31],dfFinalEscrever.DATA.unique()[-1])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "from clickhouse_driver import Client\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX')\n",
    "\n",
    "\n",
    "# Estabelecer a base a ser lida\n",
    "baseCH = dfEscrever120.copy()\n",
    "\n",
    "# Estabelecer a tabela a ser criada\n",
    "tabela = \"Delta_120dias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Unrecognized column 'Adequacao_4_Dias_Antes' in table Delta_dbd_Dia",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:85\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\clickhouse_connect\\driver\\client.py:585\u001b[0m, in \u001b[0;36mClient.insert_df\u001b[1;34m(self, table, df, database, settings, column_names, column_types, column_type_names, context)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(column_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame column count does not match insert_columns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert(table,\n\u001b[0;32m    586\u001b[0m                    df,\n\u001b[0;32m    587\u001b[0m                    column_names,\n\u001b[0;32m    588\u001b[0m                    database,\n\u001b[0;32m    589\u001b[0m                    column_types\u001b[38;5;241m=\u001b[39mcolumn_types,\n\u001b[0;32m    590\u001b[0m                    column_type_names\u001b[38;5;241m=\u001b[39mcolumn_type_names,\n\u001b[0;32m    591\u001b[0m                    settings\u001b[38;5;241m=\u001b[39msettings, context\u001b[38;5;241m=\u001b[39mcontext)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\clickhouse_connect\\driver\\client.py:543\u001b[0m, in \u001b[0;36mClient.insert\u001b[1;34m(self, table, data, column_names, database, column_types, column_type_names, column_oriented, settings, context)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo data specified for insert\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_insert_context(table,\n\u001b[0;32m    544\u001b[0m                                          column_names,\n\u001b[0;32m    545\u001b[0m                                          database,\n\u001b[0;32m    546\u001b[0m                                          column_types,\n\u001b[0;32m    547\u001b[0m                                          column_type_names,\n\u001b[0;32m    548\u001b[0m                                          column_oriented,\n\u001b[0;32m    549\u001b[0m                                          settings)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\clickhouse_connect\\driver\\client.py:653\u001b[0m, in \u001b[0;36mClient.create_insert_context\u001b[1;34m(self, table, column_names, database, column_types, column_type_names, column_oriented, settings, data)\u001b[0m\n\u001b[0;32m    651\u001b[0m             column_types \u001b[38;5;241m=\u001b[39m [column_map[name]\u001b[38;5;241m.\u001b[39mch_type \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_names]\n\u001b[0;32m    652\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 653\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(column_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(column_types):\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn names do not match column types\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: Unrecognized column 'Adequacao_4_Dias_Antes' in table Delta_dbd_Dia"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Estabelecer a base a ser lida\n",
    "\n",
    "bases = [1, 2, 3]\n",
    "\n",
    "for qualbase in bases:\n",
    "\n",
    "    if qualbase == 1:\n",
    "        baseCH = dfEscreverDia.copy()\n",
    "        tabela = \"Delta_dbd_Dia\" # BaseGoChillDia\n",
    "\n",
    "    elif qualbase == 2:\n",
    "        baseCH = dfEscreverMes.copy()\n",
    "        tabela = \"Delta_dbd_Mes\" # BaseGoChillMes\n",
    "\n",
    "    elif qualbase == 3:\n",
    "        baseCH = dfEscrever120.copy()\n",
    "        tabela = \"Delta_dbd_120dias\" # BaseDelta120\n",
    "\n",
    "    elif qualbase == 4:\n",
    "        baseCH = dfNinjas.copy()\n",
    "        tabela = \"Delta_dbd_Audit\" # BaseDeltaNinja\n",
    "\n",
    "    ## Algumas alterações\n",
    "    baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar a Data para datetime\n",
    "\n",
    "    baseCH.columns = [unidecode(col) for col in baseCH.columns]        # Tirar acentos e afins dos nomes das colunas porque \n",
    "                                                                       # o Clickhouse não gosta\n",
    "\n",
    "\n",
    "    ## Listas para definir os tipos de dados de cada coluna a inserit\n",
    "    data = [\"DATA\"]\n",
    "    texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object']\n",
    "    inteiros = [col for col in baseCH.columns if baseCH[col].dtype == 'int64' or baseCH[col].dtype == 'int32']\n",
    "    floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "    ## Mudar inteiros para floats porque senão não pode haver missing values\n",
    "    for col_name in inteiros:\n",
    "        baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "\n",
    "    ## Missing values em strings também estragam tudo\n",
    "    baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "    baseCH[texto] = baseCH[texto].astype(str)\n",
    "\n",
    "    #Função que vai fazer a schema\n",
    "    def schema(lista, tipo):\n",
    "        result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "        return result_list\n",
    "\n",
    "    # Schema a ser feito\n",
    "    data1 = schema(data, \"Date\")\n",
    "    texto1 = schema(texto, \"String\")\n",
    "    inteiros1 = schema(inteiros, \"Float64\")\n",
    "    floats1 = schema(floats, \"Float64\")\n",
    "    total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "    schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "    # Split the input string by commas\n",
    "    parts = schema.split(', ')\n",
    "    # Process each part and wrap the first word in double quotes\n",
    "    output_parts = []\n",
    "    for part in parts:\n",
    "        words = part.split()\n",
    "        if words:\n",
    "            first_word = words[0]\n",
    "            remaining_words = ' '.join(words[1:])\n",
    "            output_part = f'\"{first_word}\" {remaining_words}'\n",
    "            output_parts.append(output_part)\n",
    "    # Join the modified parts back into a string\n",
    "    schema = ', '.join(output_parts)\n",
    "\n",
    "    # Eliminar tabela que possa existir no CH com o mesmo nome\n",
    "    client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "    # Criar tabela no CH\n",
    "    client.command(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "            {schema}\n",
    "            ) ENGINE = MergeTree\n",
    "            ORDER BY (DATA)\n",
    "    ''')\n",
    "\n",
    "\n",
    "    # Exportar os dados para o clickhouse\n",
    "    client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())\n",
    "    print(f\"A base {tabela} foi exportada para o clickhouse\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      \\"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  /"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passar para csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "escrever_csv(dfEscreverDia, \"DataBaseDeltaDia\")\n",
    "escrever_csv(dfEscreverMes, \"DataBaseDeltaMes\")\n",
    "escrever_csv(dfEscrever180, \"DataBaseDelta180Dias\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
