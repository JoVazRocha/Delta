{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do código\n",
    "\n",
    "### <u>Código que gera os ficheiros para estudo do comportamento da Delta</u>\n",
    "---\n",
    "O objectivo é receber dados da Delta e devolver um conjunto de métricas para prever roturas. Devolve um ficheiro que pode entrar no código 1 para juntar aos dados dos ninjas.\n",
    "\n",
    "---\n",
    "- Inputs\n",
    "\n",
    "> __Dados completos da Delta em pastas de ficheiros__ (de azul a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "                    ou\n",
    "\n",
    "> __Ficheiro já completo__ (de vermelho a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "- Outputs\n",
    "\n",
    "> __Ficheiro com produtos em causa__ em formato Long\n",
    "\n",
    "> __Métricas novas:__\n",
    "> - Roturas de Stock e Pré-rotura\n",
    "> - Sinal\n",
    "> - Ciclos e Adequação de Stock\n",
    "> - MSA (média de sellouts 10 dias antes)\n",
    "> - STK (Stock disponível + trânsito)\n",
    "> - (Novo) Balanço médio, mediano, liberal e conservador \n",
    "> - (Novo) Dias para a rotura de stock e de prateleira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 750 ms\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import clickhouse_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:Blue\">Juntar Ficheiros</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar do Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ler o ficheiro\n",
    "\n",
    "dfClick = client.query_df('SELECT * FROM BaseDelta180')\n",
    "\n",
    "dfClick = dfClick[dfClick.DATA != dfClick.DATA.unique()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importar ficheiros novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNovo = pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Diário\\\\dia_actual.xlsb\") #xlsx\n",
    "\n",
    "\n",
    "dfNovo = dfNovo[['DATA', 'EAN', 'DESC_ARTIGO', 'STORE', \"STORE_NAME\", \n",
    "                         \"SOH\", \"INTRANSIT\", \"EXPECTED\", \"PRES_STOCK\", \"VND (D-1)\"]]\n",
    "\n",
    "# Renomear colunas e criar as do dia\n",
    "\n",
    "dfNovo = dfNovo.rename(columns={\"VND (D-1)\": \"SELLOUT_1_Dias_Antes\", \n",
    "                                \"SOH\": \"STOCK_1_Dias_Antes\",\n",
    "                                \"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "dfNovo[\"SELLOUT_1_Dias_Antes\"] = np.where(dfNovo[\"SELLOUT_1_Dias_Antes\"]<0, 0, dfNovo[\"SELLOUT_1_Dias_Antes\"])\n",
    "dfNovo['SELLOUT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['INTRANSIT'] = dfNovo.groupby([\"STORE\",\"EAN\"])['INTRANSIT_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['EXPECTED'] = dfNovo.groupby([\"STORE\",\"EAN\"])['EXPECTED_1_Dias_Antes'].shift(-1)\n",
    "dfNovo['PRES_STOCK'] = dfNovo.groupby([\"STORE\",\"EAN\"])['PRES_STOCK_1_Dias_Antes'].shift(-1)\n",
    "\n",
    "#DATETIME\n",
    "# se xlsx\n",
    "#dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], format='%d-%m-%Y') \n",
    "\n",
    "# se xlsb\n",
    "dfNovo['DATA'] = pd.to_datetime(dfNovo['DATA'], unit='d', origin=datetime.datetime(1899, 12, 30)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produtos específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Passar produtos e lojas para listas\n",
    "\n",
    "produtos = dfClick.DESC_ARTIGO.unique().tolist()\n",
    "lojas = dfClick.STORE.unique().tolist()\n",
    "\n",
    "# Alterar o dataframe para apenas incluir os produtos e lojas em causa\n",
    "\n",
    "dfNovo = dfNovo[(dfNovo[\"DESC_ARTIGO\"].isin(produtos)) & (dfNovo[\"STORE\"].isin(lojas))].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDiario = pd.concat([dfClick, dfNovo], ignore_index=True, join='outer')\n",
    "\n",
    "dfFinal = dfDiario.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vendedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVendedor=pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "dfVendedor = dfVendedor.rename(columns={\"Cód. Loja\":\"STORE\"})\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=\"Vendedor\")\n",
    "\n",
    "# Criar coluna de reposição\n",
    "dfFinal = pd.merge(dfFinal, dfVendedor[[\"STORE\",\"Vendedor\"]], how=\"left\", on = \"STORE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>Ficheiro Lido<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas interessantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ROTURA\n",
    "> - PRÉ_ROTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir coluna de rotura (se stock menor ou igual a 0 e existe Linear)\n",
    "\n",
    "dfFinal[\"ROTURA\"] = np.where((dfFinal[\"STOCK\"] <= 0) & (dfFinal[\"PRES_STOCK\"] > 0), 1, 0)\n",
    "dfFinal[\"ROTURA_1_Dias_Antes\"] = np.where((dfFinal[\"STOCK_1_Dias_Antes\"] <= 0) & (dfFinal[\"PRES_STOCK\"].shift(1) > 0), 1, 0)\n",
    "\n",
    "dfFinal[\"PRE_ROTURA\"] = (dfFinal[\"STOCK\"] < dfFinal[\"PRES_STOCK\"]).astype(int)\n",
    "dfFinal[\"PRE_ROTURA_1_Dias_Antes\"] = (dfFinal[\"STOCK_1_Dias_Antes\"] < dfFinal[\"PRES_STOCK\"].shift(1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas 30, 60, 120 e 180 dias antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "diasMetHist = [30, 60, 120, 180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Volatilidade de Procura: <br>\n",
    "coeficiente de variação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Volatilidade_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).std())/\n",
    "                             dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de Rotura: <br>\n",
    "média de roturas $* 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Roturas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de Supply:<br>\n",
    "média de vezes que foi pedido stock $*100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sempre que é pedido abastecimento, fazer com que seja 1\n",
    "\n",
    "dfFinal[\"New_Supply\"] = np.where((dfFinal[\"EXPECTED\"].shift(1)==0) & (dfFinal[\"EXPECTED\"]>0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Supply_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Efeito fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfFinal['SELLOUT_fds'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([4,5,6])][\"SELLOUT\"].copy()\n",
    "#dfFinal['SELLOUT_semana'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([0,1,2,3])][\"SELLOUT\"].copy()\n",
    "dfFinal['SELLOUT_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"SELLOUT_1_Dias_Antes\"].copy()\n",
    "dfFinal['SELLOUT_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"SELLOUT_1_Dias_Antes\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Efeito_Fds_{i}\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())/\n",
    "                                              (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "                                               .transform(lambda x: x.rolling(window=i, min_periods=1).mean())))-1)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo médio inter-supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"InterSupply\"] = np.where(dfFinal[\"EXPECTED\"]==0, 1, 0)\n",
    "\n",
    "groups = (dfFinal['InterSupply'] != dfFinal['InterSupply'].shift()).cumsum()\n",
    "result = dfFinal.groupby(groups).agg({'DATA': 'first', 'DESC_ARTIGO': 'first', 'STORE': 'first', 'InterSupply': 'sum'}).reset_index(drop=True)\n",
    "result = result[result['InterSupply'] > 0]\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=['InterSupply'])\n",
    "\n",
    "dfFinal = pd.merge(dfFinal, result, how=\"left\", on=[\"DATA\",\"DESC_ARTIGO\", \"STORE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_InterSupplyMed_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo indisponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''O que vai acontecer é: no primeiro dia em que há rotura vai aparecer a soma de todos os dias com rotura a seguir a esse!\n",
    "Todos os outros valores serão NaN para não serem considerados quando for feita a média. Assim a média corresponderá ao\n",
    "número médio de dias em que se deixa um produto em rotura.'''\n",
    "\n",
    "dfFinal[\"Dias_Indisponivel_1_Dias_Antes\"] = np.where(dfFinal[\"ROTURA_1_Dias_Antes\"]==1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Dias_Indisponivel_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Dias_Indisponivel_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias em Stock Borderline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Percentagem_Stock_Borderline_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<0.2*dfFinal[\"PRES_STOCK\"].shift(1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Dias_Stock_Borderline_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Stock_Borderline_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias de Linear Incompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Percentagem_Linear_Incompleto_1_Dias_Antes\"] = np.where(dfFinal[\"STOCK_1_Dias_Antes\"]<dfFinal[\"PRES_STOCK\"].shift(1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Dias_Linear_Incompleto_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Percentagem_Linear_Incompleto_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias sem vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Sem_Vendas_1_Dias_Antes\"] = np.where(dfFinal[\"SELLOUT_1_Dias_Antes\"] == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Percentagem_Dias_Sem_Vendas_{i}\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Vendas perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['ROTURA_fds_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([5,6,0])][\"ROTURA_1_Dias_Antes\"].copy()\n",
    "dfFinal['ROTURA_semana_1_Dias_Antes'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([1,2,3,4])][\"ROTURA_1_Dias_Antes\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in diasMetHist:\n",
    "    dfFinal[f\"Vendas_Perdidas_em_{i}_Dias\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum())\\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).median())) \\\n",
    "                                                  + \n",
    "                                               (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).sum()) \\\n",
    "                                                  * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes'].transform(lambda x: x.rolling(window=i, min_periods=1).median())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas até 10 dias antes:\n",
    "\n",
    "- INSTRANSIT\n",
    "- EXPECTED\n",
    "- SELLOUT\n",
    "- CICLOS\n",
    "- Dias para Rotura\n",
    "- Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos dias antes:\n",
    "\n",
    "diaI=4         #dia inicial\n",
    "diaF=5       #dia final\n",
    "\n",
    "diasMet = [1, 4, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para colunas de dias anteriores\n",
    "def dias(df, dia, coluna):         #dia é quantos dias antes\n",
    "    a=int(dia)\n",
    "\n",
    "    valores = df.groupby(['DESC_ARTIGO', 'STORE'])[coluna].transform(lambda x: x.shift(a))\n",
    "    valores[:a] = np.nan\n",
    "    \n",
    "    df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - SELLOUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 172 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Usar função para sellouts até 10 dias antes\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"SELLOUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar função para Stocks até 10 dias antes\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"STOCK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > - Ordenar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "previsões = 2\n",
    "dfOrg = dfFinal.iloc[:, np.r_[:9, 10, 12, 13, 12+previsões, 13+previsões, 9, \n",
    "                                (23+previsões):(32+previsões), 11, (14+previsões):(23+previsões)]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_columns = dfOrg.columns.tolist()\n",
    "indexed_columns = [f\"{index}: {column}\" for index, column in enumerate(df_columns)]\n",
    "indexed_columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfFinal = dfFinal.iloc[:, np.r_[:9, 10, 12, 13, 12+previsões, 13+previsões, 9, \n",
    "                                (23+previsões):(32+previsões), 11, (14+previsões):(23+previsões)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - INTRANSIT e EXPECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar função para Trânsito até 10 dias antes\n",
    "\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"INTRANSIT\")\n",
    "    \n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"EXPECTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STK do dia = soma dos stocks em loja com os stocks em trânsito no próprio dia\n",
    "\n",
    "dfFinal[\"STK\"] = dfFinal[\"STOCK\"] + dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"STK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA do dia = média dos sellouts dos 10 dias anteriores ao dia em causa\n",
    "\n",
    "dfFinal[\"MSA10\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "dfFinal[\"MSA10Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=10, min_periods=1).std())\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA10\")\n",
    "\n",
    "    \n",
    "dfFinal[\"MSA20\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n",
    "dfFinal[\"MSA20Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=20, min_periods=1).std())\n",
    "  \n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"MSA20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - CICLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluna de Ciclos de reposição\n",
    "\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"STOCK_1_Dias_Antes\"]/dfFinal[\"PRES_STOCK\"]\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"CICLOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Adequação de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joao_\\AppData\\Local\\Temp\\ipykernel_5688\\4171257849.py:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores\n",
      "C:\\Users\\joao_\\AppData\\Local\\Temp\\ipykernel_5688\\4171257849.py:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores\n",
      "C:\\Users\\joao_\\AppData\\Local\\Temp\\ipykernel_5688\\4171257849.py:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores\n",
      "C:\\Users\\joao_\\AppData\\Local\\Temp\\ipykernel_5688\\4171257849.py:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores\n"
     ]
    }
   ],
   "source": [
    "# Coluna de Adequacao de stock\n",
    "\n",
    "\n",
    "dfFinal[\"Adequacao\"]= np.where(dfFinal[\"CICLOS\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]>=dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]<dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "\n",
    "for i in diasMet:\n",
    "    dias(dfFinal, i, \"Adequacao\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Smart é com semana e fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.66 s\n",
      "Wall time: 9.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Define a function to calculate weekday and weekend counts\n",
    "def calculate_weekday_weekend_counts(date):\n",
    "    weekday_count = 0\n",
    "    weekend_count = 0\n",
    "    \n",
    "    for _ in range(5):\n",
    "        if date.weekday() < 4:  # Monday to Thursday\n",
    "            weekday_count += 1\n",
    "        else:  # Friday to Sunday\n",
    "            weekend_count += 1\n",
    "        \n",
    "        date += timedelta(days=1)\n",
    "    \n",
    "    return pd.Series({\"CONTAGEM_SEMANA\": weekday_count, \"CONTAGEM_FIMSEMANA\": weekend_count})\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "dfFinal[[\"CONTAGEM_SEMANA\", \"CONTAGEM_FIMSEMANA\"]] = dfFinal[\"DATA\"].apply(calculate_weekday_weekend_counts)\n",
    "\n",
    "\n",
    "\n",
    "# 5 dias antes\n",
    "dfFinal[\"Balance_Smart\"] = ((\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])\n",
    "    +\n",
    "    (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA']))\n",
    "     / dfFinal[\"STK_1_Dias_Antes\"])\n",
    "dfFinal[\"Balance_Smart\"] = np.where(dfFinal[\"STK\"]<=0, -1, dfFinal[\"Balance_Smart\"])\n",
    "dfFinal[\"Balance_Smart_5_Dias_Antes\"] = dfFinal[\"Balance_Smart\"].shift(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"RISCO\"] = np.where((dfFinal.Balance_Smart > 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.2) & (dfFinal.Percentagem_Roturas_120 > 5), 1,\n",
    "                   np.where((dfFinal.Balance_Smart > 1) & (dfFinal.Balance_Smart < 2) & (dfFinal.CICLOS_5_Dias_Antes < 0.6) & (dfFinal.Percentagem_Roturas_120 < 3), 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"RISCO_ROTURA\"] = np.where((dfFinal.Balance_Smart >= 12.5) | (dfFinal.Balance_Smart < 0), 1, \n",
    "                          np.where((dfFinal.Balance_Smart >= 1.1) & (dfFinal.Balance_Smart < 12.5), 2,\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.833) & (dfFinal.Balance_Smart < 1.1), 3,\n",
    "                          np.where((dfFinal.Balance_Smart >= 0.087) & (dfFinal.Balance_Smart < 0.833), 4, 5))))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = dfFinal.copy()\n",
    "# Dias para a rotura mas com o Sellout médio (móvel) dos últimos 10 dias \n",
    "dfFinal[\"Dias_para_Rotura_Stock\"] = dfFinal[\"STOCK_1_Dias_Antes\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_para_Rotura_Stock_5_Dias_Antes\"] = dfFinal[\"Dias_para_Rotura_Stock\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a métrica: Preslinear / med(Sellouts 10 dias)\n",
    "dfFinal['Dias_Duracao_Linear'] = dfFinal[\"PRES_STOCK\"] / (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_FIMSEMANA'])+(dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana_1_Dias_Antes']\\\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=1).mean())* dfFinal['CONTAGEM_SEMANA'])\n",
    "\n",
    "dfFinal[\"Dias_Duracao_Linear_5_Dias_Antes\"] = dfFinal[\"Dias_Duracao_Linear\"].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Escrever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dias certos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ficheiro Dia\n",
    "BaseDeltaDia = dfFinal[dfFinal.DATA == dfFinal.DATA.unique()[-1]].copy()\n",
    "\n",
    "# Ficheiro Mês\n",
    "BaseDeltaMes = dfFinal[dfFinal.DATA >= dfFinal.DATA.unique()[-30]].copy()\n",
    "\n",
    "# Ficheiro 180 dias\n",
    "BaseDelta180Dias = dfFinal[dfFinal.DATA >= dfFinal.DATA.unique()[-180]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfFinal.DATA.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passar para csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "from clickhouse_driver import Client\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Estabelecer a base a ser lida\n",
    "\n",
    "\n",
    "Bases = [BaseDeltaDia, BaseDeltaMes, BaseDelta180Dias]\n",
    "\n",
    "baseDay = BaseDeltaDia\n",
    "tabela = BaseDeltaDia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>DESC_ARTIGO</th>\n",
       "      <th>STORE_NAME</th>\n",
       "      <th>Adequacao</th>\n",
       "      <th>Adequacao_1_Dias_Antes</th>\n",
       "      <th>Adequacao_4_Dias_Antes</th>\n",
       "      <th>Adequacao_5_Dias_Antes</th>\n",
       "      <th>Adequacao_10_Dias_Antes</th>\n",
       "      <th>EAN</th>\n",
       "      <th>STORE</th>\n",
       "      <th>...</th>\n",
       "      <th>Balance_Mediano_5_Dias_Antes</th>\n",
       "      <th>Balance_Mediano_10_Dias_Antes</th>\n",
       "      <th>Balance_Smart</th>\n",
       "      <th>Balance_Smart_5_Dias_Antes</th>\n",
       "      <th>Dias_para_Rotura_Stock</th>\n",
       "      <th>Dias_para_Rotura_Stock_5_Dias_Antes</th>\n",
       "      <th>Dias_Duracao_Linear</th>\n",
       "      <th>Dias_Duracao_Linear_5_Dias_Antes</th>\n",
       "      <th>Vendedor</th>\n",
       "      <th>InterSupply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33870</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>CAFÉ SOLÚVEL DELTA FRASCO 100G</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>5.609060e+12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.730594</td>\n",
       "      <td>8.647059</td>\n",
       "      <td>27.881279</td>\n",
       "      <td>Carla Teixeira</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33871</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>CAFÉ SOLÚVEL DELTA FRASCO 100G</td>\n",
       "      <td>CNT AMADORA</td>\n",
       "      <td></td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>5.609060e+12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.318507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.034188</td>\n",
       "      <td>10.387016</td>\n",
       "      <td>25.914530</td>\n",
       "      <td>Brizida Almeida</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33872</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>CAFÉ SOLÚVEL DELTA FRASCO 100G</td>\n",
       "      <td>CNT CASCAIS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>Stock Insuf c Forn Adequado</td>\n",
       "      <td>Stock Insuf c Forn Desadequado</td>\n",
       "      <td>5.609060e+12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.846154</td>\n",
       "      <td>8.018913</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>Brizida Almeida</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33873</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>CAFÉ SOLÚVEL DELTA FRASCO 100G</td>\n",
       "      <td>CNT LEIRIA</td>\n",
       "      <td></td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>5.609060e+12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.403828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.267771</td>\n",
       "      <td>11.248366</td>\n",
       "      <td>66.752243</td>\n",
       "      <td>Filipa Dias</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33874</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>CAFÉ SOLÚVEL DELTA FRASCO 100G</td>\n",
       "      <td>CNT COIMBRASHOPPING</td>\n",
       "      <td></td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>Stock Suficiente</td>\n",
       "      <td>5.609060e+12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.818182</td>\n",
       "      <td>8.647059</td>\n",
       "      <td>Ana Marques</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DATA                     DESC_ARTIGO           STORE_NAME  \\\n",
       "33870 2023-08-18  CAFÉ SOLÚVEL DELTA FRASCO 100G       CNT MATOSINHOS   \n",
       "33871 2023-08-18  CAFÉ SOLÚVEL DELTA FRASCO 100G          CNT AMADORA   \n",
       "33872 2023-08-18  CAFÉ SOLÚVEL DELTA FRASCO 100G          CNT CASCAIS   \n",
       "33873 2023-08-18  CAFÉ SOLÚVEL DELTA FRASCO 100G           CNT LEIRIA   \n",
       "33874 2023-08-18  CAFÉ SOLÚVEL DELTA FRASCO 100G  CNT COIMBRASHOPPING   \n",
       "\n",
       "      Adequacao Adequacao_1_Dias_Antes          Adequacao_4_Dias_Antes  \\\n",
       "33870                                                 Stock Suficiente   \n",
       "33871                 Stock Suficiente                Stock Suficiente   \n",
       "33872                                   Stock Insuf c Forn Desadequado   \n",
       "33873                 Stock Suficiente                Stock Suficiente   \n",
       "33874                 Stock Suficiente                Stock Suficiente   \n",
       "\n",
       "            Adequacao_5_Dias_Antes         Adequacao_10_Dias_Antes  \\\n",
       "33870             Stock Suficiente                Stock Suficiente   \n",
       "33871             Stock Suficiente                Stock Suficiente   \n",
       "33872  Stock Insuf c Forn Adequado  Stock Insuf c Forn Desadequado   \n",
       "33873             Stock Suficiente                Stock Suficiente   \n",
       "33874             Stock Suficiente                Stock Suficiente   \n",
       "\n",
       "                EAN  STORE  ...  Balance_Mediano_5_Dias_Antes  \\\n",
       "33870  5.609060e+12    1.0  ...                           NaN   \n",
       "33871  5.609060e+12    2.0  ...                           NaN   \n",
       "33872  5.609060e+12    3.0  ...                           NaN   \n",
       "33873  5.609060e+12    5.0  ...                           NaN   \n",
       "33874  5.609060e+12    6.0  ...                           NaN   \n",
       "\n",
       "       Balance_Mediano_10_Dias_Antes  Balance_Smart  \\\n",
       "33870                            NaN            NaN   \n",
       "33871                            NaN            NaN   \n",
       "33872                            NaN            NaN   \n",
       "33873                            NaN            NaN   \n",
       "33874                            NaN            NaN   \n",
       "\n",
       "       Balance_Smart_5_Dias_Antes  Dias_para_Rotura_Stock  \\\n",
       "33870                    0.509306                     NaN   \n",
       "33871                    0.318507                     NaN   \n",
       "33872                    0.402564                     NaN   \n",
       "33873                    0.403828                     NaN   \n",
       "33874                         NaN                     NaN   \n",
       "\n",
       "       Dias_para_Rotura_Stock_5_Dias_Antes  Dias_Duracao_Linear  \\\n",
       "33870                            27.730594             8.647059   \n",
       "33871                            28.034188            10.387016   \n",
       "33872                             7.846154             8.018913   \n",
       "33873                            68.267771            11.248366   \n",
       "33874                                  NaN             9.818182   \n",
       "\n",
       "       Dias_Duracao_Linear_5_Dias_Antes         Vendedor  InterSupply  \n",
       "33870                         27.881279   Carla Teixeira         13.0  \n",
       "33871                         25.914530  Brizida Almeida          NaN  \n",
       "33872                          8.307692  Brizida Almeida          NaN  \n",
       "33873                         66.752243      Filipa Dias          NaN  \n",
       "33874                          8.647059      Ana Marques          NaN  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseDeltaDia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estabelecer a base a ser lida\n",
    "\n",
    "baseCH = BaseDeltaDia.copy()\n",
    "#2023\n",
    "tabela = \"BaseDeltaDia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x1c91c51d180>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "###\n",
    "baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar para datetime\n",
    "new_columns = [unidecode(col) for col in baseCH.columns]           # Tirar acentos e afins\n",
    "baseCH.columns = new_columns                                       # Aplicar alterações da linha anterior\n",
    "###\n",
    "\n",
    "# Tipos de dados\n",
    "data = [\"DATA\"]\n",
    "texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object']\n",
    "inteiros = [col for col in baseCH.columns if baseCH[col].dtype == 'int64' or baseCH[col].dtype == 'int32']\n",
    "floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "# Só floats é que permitem missing values\n",
    "for col_name in inteiros:\n",
    "    baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "# Missing values em strings estragam tudo\n",
    "baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "\n",
    "\n",
    "def schema(lista, tipo):\n",
    "    result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "    return result_list\n",
    "\n",
    "data1 = schema(data, \"Date\")\n",
    "texto1 = schema(texto, \"String\")\n",
    "inteiros1 = schema(inteiros, \"Float64\")\n",
    "floats1 = schema(floats, \"Float64\")\n",
    "total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "\n",
    "schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "# Split the input string by commas\n",
    "parts = schema.split(', ')\n",
    "# Process each part and wrap the first word in double quotes\n",
    "output_parts = []\n",
    "for part in parts:\n",
    "    words = part.split()\n",
    "    if words:\n",
    "        first_word = words[0]\n",
    "        remaining_words = ' '.join(words[1:])\n",
    "        output_part = f'\"{first_word}\" {remaining_words}'\n",
    "        output_parts.append(output_part)\n",
    "# Join the modified parts back into a string\n",
    "schema = ', '.join(output_parts)\n",
    "\n",
    "\n",
    "\n",
    "# Eliminar tabela no CH\n",
    "client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "# Criar tabela no CH\n",
    "client.command(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "        {schema}\n",
    "        ) ENGINE = MergeTree\n",
    "        ORDER BY (DATA)\n",
    "''')\n",
    "\n",
    "client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for Base in Bases:\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "    Base['DATA']= pd.to_datetime(Base['DATA'], format='%Y-%m-%d')  # Passar para datetime\n",
    "    new_columns = [unidecode(col) for col in Base.columns]           # Tirar acentos e afins\n",
    "    Base.columns = new_columns                                       # Aplicar alterações da linha anterior\n",
    "    ###\n",
    "\n",
    "    # Tipos de dados\n",
    "    data = [\"DATA\"]\n",
    "    texto = [col for col in Base.columns if Base[col].dtype == 'object']\n",
    "    inteiros = [col for col in Base.columns if Base[col].dtype in ['int64', 'int32']]\n",
    "    floats = [col for col in Base.columns if Base[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "    # Só floats é que permitem missing values\n",
    "    for col_name in inteiros:\n",
    "        Base[col_name] = Base[col_name].astype(float)\n",
    "    # Missing values em strings estragam tudo\n",
    "    Base[texto] = Base[texto].fillna(\"-\")\n",
    "\n",
    "\n",
    "    def schema(lista, tipo):\n",
    "        result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "        return result_list\n",
    "\n",
    "    data1 = schema(data, \"Date\")\n",
    "    texto1 = schema(texto, \"String\")\n",
    "    inteiros1 = schema(inteiros, \"Float64\")\n",
    "    floats1 = schema(floats, \"Float64\")\n",
    "    total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "\n",
    "    schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "    # Split the input string by commas\n",
    "    parts = schema.split(', ')\n",
    "    # Process each part and wrap the first word in double quotes\n",
    "    output_parts = []\n",
    "    for part in parts:\n",
    "        words = part.split()\n",
    "        if words:\n",
    "            first_word = words[0]\n",
    "            remaining_words = ' '.join(words[1:])\n",
    "            output_part = f'\"{first_word}\" {remaining_words}'\n",
    "            output_parts.append(output_part)\n",
    "    # Join the modified parts back into a string\n",
    "    schema = ', '.join(output_parts)\n",
    "\n",
    "\n",
    "\n",
    "    # Eliminar Base no CH\n",
    "    client.command(f'DROP TABLE IF EXISTS {Base}')\n",
    "\n",
    "    # Criar Base no CH\n",
    "    client.command(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {Base} (\n",
    "            {schema}\n",
    "            ) ENGINE = MergeTree\n",
    "            ORDER BY (DATA)\n",
    "    ''')\n",
    "\n",
    "    client.insert_df(Base, Base, column_names=Base.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Path to the JSON credentials file\n",
    "CREDENTIALS_FILE = 'D:\\\\B&N Dados\\\\Delta\\\\Diário\\\\drive.json'\n",
    "# Unique ID of the Google Drive folder you want to list\n",
    "FOLDER_ID = '19sDhlgXLOvvJ5mfeGA67cd-bylg5S6OB'  # Replace with the actual folder ID\n",
    "\n",
    "# Initialize authentication\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CREDENTIALS_FILE, scopes=['https://www.googleapis.com/auth/drive']\n",
    ")\n",
    "# Create an instance of the Google Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "# List of files in the folder\n",
    "results = drive_service.files().list(\n",
    "    q=f\"'{FOLDER_ID}' in parents and trashed=false\",\n",
    "    fields=\"files(id, name)\").execute()\n",
    "# Iterate through the files and build the list of URLs and names\n",
    "file_list = []\n",
    "for file in results.get('files', []):\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    file_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    file_list.append({'name': file_name, 'url': file_url})\n",
    "# Print the list of URLs and names\n",
    "for item in file_list:\n",
    "    print(f'Nome: {item[\"name\"]}')\n",
    "    print(f'URL: {item[\"url\"]}')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
