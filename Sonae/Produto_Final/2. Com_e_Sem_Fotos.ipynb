{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Só Ninjas</center>\n",
    "\n",
    "# <center><u>``Resumo do código``</u></center>\n",
    "\n",
    "### <u>Código que pega nos ficheiros da Sonae e cria </u>\n",
    "\n",
    ">### Entra: \n",
    "> - 1_Ninjas\n",
    "\n",
    ">### Sai: \n",
    "> - Com Fotos\n",
    "> - Sem Fotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "from ipywidgets import interact\n",
    "\n",
    "def escrever_excel(dfa, nome):\n",
    "    dfa.to_excel(f'C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\2.Delta\\\\Sonae\\\\Testes\\\\{nome}.xlsx', index=False)\n",
    "    \n",
    "def escrever_csv(dfa, nome):\n",
    "    dfa.to_csv(f'C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\2.Delta\\\\Sonae\\\\Testes\\\\{nome}.csv', index=False)\n",
    "    \n",
    "def escrever_txt(dfa, nome):\n",
    "    dfa.to_csv(f'C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\2.Delta\\\\Sonae\\\\Testes\\\\{nome}.txt', sep='\\t', index=False, header=False)\n",
    "    \n",
    "def ler_json(ficheiro):\n",
    "    with open(f'C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\2.Delta\\\\Sonae\\\\Testes\\\\{ficheiro}.json', 'r') as file:\n",
    "        mapa = json.load(file)\n",
    "    return mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section0\"></a>\n",
    "\n",
    "# Índice\n",
    "\n",
    "1. [Data Cleaning](#section1)<br>\n",
    "2. [DataFusion Long](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <center>1. Data Cleaning</center>\n",
    "[Voltar ao índice](#section0)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # `Dicionários super úteis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ler os dicionários previamente criados para poder usar os códigos de produto e de loja em vez dos nomes '''\n",
    "\n",
    "mapa_produtos = ler_json(\"dicionário_produtos\")\n",
    "mapa_produtos = {int(key): value for key, value in mapa_produtos.items()}\n",
    "\n",
    "mapa_produtos2 = ler_json(\"dicionário_produtos_23\")\n",
    "mapa_produtos2 = {int(key): value for key, value in mapa_produtos2.items()} \n",
    "\n",
    "mapa_lojas = ler_json(\"dicionário_lojas\")\n",
    "mapa_lojas = {int(key): value for key, value in mapa_lojas.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # `Ninjas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=blue>Ler</font>\n",
    "<a id=\"section1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 926 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ler o ficheiro long com Stocks e Fornecimento\n",
    "\n",
    "directória = \"C:\\\\Users\\\\Chip7\\\\Desktop\\\\B&N\\\\2.Delta\\\\Sonae\\\\Diario\\\\\"\n",
    "num_bases = 1\n",
    "\n",
    "prefixo = \"1_Ninjas\"\n",
    "extensão = \".xlsx\" #xlsb\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for i in range(1, num_bases + 1):\n",
    "    file_path = f\"{directória}{prefixo}{i}{extensão}\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    dataframes.append(df)\n",
    "    \n",
    "dfNinjas = pd.concat(dataframes, ignore_index=True)\n",
    "dfNinjas = dfNinjas.rename(columns={'Data de Resposta': 'DATA', \"Código da loja\":\"STORE\", \"Nome da Loja\":\"STORE_NAME\"})\n",
    "\n",
    "\n",
    "# Quando tiver a porcaria dos EANS\n",
    "#dfNinjas = dfNinjas.rename(columns=lambda x: mapa_produtos2.get(x, x))\n",
    "\n",
    "\"\"\"Datetime\"\"\"\n",
    "\n",
    "# se xlsx\n",
    "dfNinjas['DATA'] = pd.to_datetime(dfNinjas['DATA'], format='%d-%m-%Y') \n",
    "\n",
    "# se xlsb\n",
    "#dfNinjas['DATA'] = pd.to_datetime(dfNinjas['DATA'], unit='d', origin=datetime.datetime(1899, 12, 30)) \n",
    "\n",
    "produtos = dfNinjas.columns[3:].tolist()\n",
    "dfNinjas = dfNinjas.dropna(how = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Lojas, produtos e dicionário de lojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lojas = dfNinjas.STORE_NAME.unique().tolist()\n",
    "codlojas = dfNinjas.STORE.unique().tolist()\n",
    "datas = dfNinjas.DATA.unique()\n",
    "\n",
    "numprod = len(produtos)\n",
    "\n",
    "\n",
    "ficheiroProdutos = pd.DataFrame(produtos, columns=[\"Produtos\"])\n",
    "ficheiroLojas = pd.DataFrame(lojas, columns=[\"Produtos\"])\n",
    "dicionarioLojas = dict(zip(dfNinjas['STORE'].unique(), dfNinjas['STORE_NAME'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Número de produtos presentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNinjas[\"Num_Produtos\"] = dfNinjas[produtos].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dia da Semana e Fim de Semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNinjas['Dia_da_Semana'] = dfNinjas['DATA'].dt.day_name().map({'Friday': 'Sexta', \n",
    "                                                                'Saturday': 'Sábado', \n",
    "                                                                'Sunday': 'Domingo'})\n",
    "\n",
    "dfNinjas['FDS'] = dfNinjas['DATA'].dt.isocalendar().week\n",
    "dfNinjas['FDS'] = dfNinjas.groupby('FDS').ngroup() + 1\n",
    "\n",
    "mapping = {1: \"9-10 Setembro\",\n",
    "           2: \"16-17 Setembro\",\n",
    "           3: \"23-24 Setembro\",\n",
    "           4: \"30-01 Setembro/Outubro\"}\n",
    "dfNinjas[\"Order_FDS\"] = dfNinjas[\"FDS\"]\n",
    "dfNinjas[\"FDS\"] = dfNinjas[\"FDS\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0: DATA',\n",
       " '1: STORE_NAME',\n",
       " '2: STORE',\n",
       " '3: Num_Produtos',\n",
       " '4: Dia_da_Semana',\n",
       " '5: FDS',\n",
       " '6: Order_FDS',\n",
       " '7: CAFFELATTE GO CHILL DELTA 230 ML',\n",
       " '8: CAPPUCCINO GO CHILL DELTA 230 ML',\n",
       " '9: DOUBLE ESPRESSO GO CHILL DELTA 230ML',\n",
       " '10: CARAMEL MACCHIATO GO CHILL DELTA 230ML',\n",
       " '11: LATTE MACCHIATO GO CHILL S/LAC 230ML']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{index}: {column}\" for index, column in enumerate(dfOrganizado.columns.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNinjas = dfNinjas.iloc[:, np.r_[2,0:2,8:12,3:8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <center> 2. DataFusion Long</center>\n",
    "[Voltar ao índice](#section0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #  `dfFinal1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resto = dfNinjas.columns.tolist()[:7]\n",
    "# Passar ficheiro ninjas para long\n",
    "dfNinjasLong = dfNinjas.melt(id_vars=resto, value_vars=produtos, var_name='DESC_ARTIGO', value_name='NinjaInfo')\n",
    "\n",
    "\n",
    "#Mergir\n",
    "#dfMeio = pd.merge(dfNinjasLong, dfDelta, how=\"left\", on = [\"DATA\",\"STORE\", \"DESC_ARTIGO\"]) \n",
    "#dfMeio = dfMeio.drop(columns=[\"Loja\"])\n",
    "\n",
    "#dfFinal1=dfMeio.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluna de rotura \n",
    "dfNinjasLong[\"Rotura_Linear\"] = np.where(dfNinjasLong.NinjaInfo == 0, 1, 0)\n",
    "\n",
    "# Coluna de Roturas Consecutivas cheia de zeros\n",
    "dfNinjasLong[\"Rotura_Consecutiva\"] = 0\n",
    "\n",
    "# Criar a máscar para depois aplicar a função nos grupos estabelecidos\n",
    "mask = ((dfNinjasLong[\"Rotura_Linear\"] == 1) &\n",
    "        (dfNinjasLong.groupby([\"FDS\", \"STORE\", \"DESC_ARTIGO\"])[\"Rotura_Linear\"].shift(1) == 1))\n",
    "\n",
    "# Usar a função soma para os grupos da máscara\n",
    "dfNinjasLong[\"Rotura_Consecutiva\"] = mask.groupby([dfNinjasLong[\"FDS\"], dfNinjasLong[\"STORE\"], dfNinjasLong[\"DESC_ARTIGO\"]]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "ids = pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Padrão\\\\GoChill\\\\idsGoChill.xlsx\")\n",
    "ids = ids.ID.tolist()\n",
    "\n",
    "url = []\n",
    "for i in ids:\n",
    "    url.append(f\"https://app.brandsandninjas.com/answer_photos.csv?id={i}\")\n",
    "\n",
    "dataframes = []\n",
    "for i in url:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(i)  # Use the current URL 'i' here\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the response\n",
    "        file_content = response.content\n",
    "\n",
    "        # Create a Pandas DataFrame from the CSV content\n",
    "        df = pd.read_csv(StringIO(file_content.decode('utf-8')))\n",
    "        \n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "dfFotos = pd.concat(dataframes, ignore_index=True)\n",
    "dfFotos = dfFotos.rename(columns = {\"photo_taken_at\":\"DATA\", \"store_name\":\"STORE_NAME\"})\n",
    "dfFotos['Hora_da_Foto'] = dfFotos['DATA'].str.split(' ').str[1]\n",
    "dfFotos['DATA_Foto'] = dfFotos['DATA']\n",
    "dfFotos['DATA'] = dfFotos['DATA'].str.split(' ').str[0]\n",
    "dfFotos['DATA'] = pd.to_datetime(dfFotos['DATA'], format='%Y-%m-%d')\n",
    "\n",
    "dicionárioLatLong = dict(zip(dfFotos['STORE_NAME'], dfFotos['location']))\n",
    "\n",
    "\n",
    "dfFotos[\"photo_url2\"] = dfFotos[\"photo_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionarioCoordenadas = dict(zip(dfFotos['STORE_NAME'], dfFotos['location']))\n",
    "dfNinjasLong[\"STORE_NAME2\"] = dfNinjasLong[\"STORE_NAME\"]\n",
    "dfNinjasLong[\"Color\"] = dfNinjasLong[\"Rotura_Linear\"].map({0:\"Produto Presente\",1:\"Produto em Rotura\"})\n",
    "dfNinjasLong[\"LatLong\"] = dfNinjasLong[\"STORE_NAME\"].map(dicionarioCoordenadas)\n",
    "dfNinjasLong[\"Percentagem_de_Presença\"] = dfNinjasLong[\"Num_Produtos\"]/len(produtos)\n",
    "id_Dia_da_Semana = {'Segunda': 1,\n",
    "                    'Terça': 2,\n",
    "                    'Quarta': 3,\n",
    "                    'Quinta': 4,\n",
    "                    'Sexta': 5,\n",
    "                    'Sábado': 6,\n",
    "                    'Domingo': 7}\n",
    "\n",
    "\n",
    "dfNinjasLong[\"Order_Dia\"] = dfNinjasLong[\"Dia_da_Semana\"].map(id_Dia_da_Semana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNinjasLong['DATA'] = pd.to_datetime(dfNinjasLong['DATA'], format='%Y-%m-%d')\n",
    "dfNinjasLong['DATA'] = dfNinjasLong['DATA'].dt.strftime(\"%Y-%m-%d\")\n",
    "dfNinjasLong['DATA'] = pd.to_datetime(dfNinjasLong['DATA'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFotos2 =  pd.merge(dfNinjasLong, dfFotos, how=\"left\", on = [\"DATA\",\"STORE_NAME\"]) \n",
    "dfFotos2 = dfFotos2.sort_values(by=[\"STORE\",\"DESC_ARTIGO\",\"DATA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionarioCodLojas = {v: k for k, v in dicionarioLojas.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFotos2[\"STORE\"] = dfFotos2[\"STORE_NAME\"].map(dicionarioCodLojas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler ficheiro\n",
    "dfVendedor=pd.read_excel(\"D:\\\\B&N Dados\\\\Delta\\\\Vendedor2.xlsx\", sheet_name = \"Lojas Sonae para o desafio\")\n",
    "dfVendedor = dfVendedor.rename(columns={\"Cód. Loja\":\"STORE\"})\n",
    "\n",
    "# Criar coluna de reposição\n",
    "dfNinjasLong = pd.merge(dfNinjasLong, dfVendedor[[\"STORE\",\"Vendedor\"]], how=\"left\", on = \"STORE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFotos2 = pd.merge(dfFotos2, dfVendedor[[\"STORE\",\"Vendedor\"]], how=\"left\", on = \"STORE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "from clickhouse_driver import Client\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX',\n",
    "                                       database = \"Delta\")\n",
    "\n",
    "qualbase = 2\n",
    "\n",
    "if qualbase == 1:\n",
    "    baseCH = dfNinjasLong.copy()\n",
    "    tabela = \"Delta_SemFotos\" # GoChill4SemFotos2\n",
    "\n",
    "if qualbase == 2:\n",
    "    baseCH = dfFotos2.copy()\n",
    "    tabela = \"Delta_ComFotos\" # GoChill4ComFotos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x29bb6ddeb90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Algumas alterações\n",
    "baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar a Data para datetime\n",
    "\n",
    "baseCH.columns = [unidecode(col) for col in baseCH.columns]        # Tirar acentos e afins dos nomes das colunas porque \n",
    "                                                                   # o Clickhouse não gosta\n",
    "\n",
    "\n",
    "## Listas para definir os tipos de dados de cada coluna a inserir\n",
    "\n",
    "data = [\"DATA\"]\n",
    "#data = [col for col in baseCH.columns if baseCH[col].dtype == 'datetime64[ns]']\n",
    "texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object']\n",
    "inteiros = [col for col in baseCH.columns if baseCH[col].dtype == 'int64' or baseCH[col].dtype == 'int32']\n",
    "floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "## Mudar inteiros para floats porque senão não pode haver missing values\n",
    "for col_name in inteiros:\n",
    "    baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "\n",
    "## Missing values em strings também estragam tudo\n",
    "baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "baseCH[texto] = baseCH[texto].astype(str)\n",
    "\n",
    "#Função que vai fazer a schema\n",
    "def schema(lista, tipo):\n",
    "    result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "    return result_list\n",
    "\n",
    "# Schema a ser feito\n",
    "data1 = schema(data, \"Date\")\n",
    "texto1 = schema(texto, \"String\")\n",
    "inteiros1 = schema(inteiros, \"Float64\")\n",
    "floats1 = schema(floats, \"Float64\")\n",
    "total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "# Split the input string by commas\n",
    "parts = schema.split(', ')\n",
    "# Process each part and wrap the first word in double quotes\n",
    "output_parts = []\n",
    "for part in parts:\n",
    "    words = part.split()\n",
    "    if words:\n",
    "        first_word = words[0]\n",
    "        remaining_words = ' '.join(words[1:])\n",
    "        output_part = f'\"{first_word}\" {remaining_words}'\n",
    "        output_parts.append(output_part)\n",
    "# Join the modified parts back into a string\n",
    "schema = ', '.join(output_parts)\n",
    "\n",
    "# Eliminar tabela que possa existir no CH com o mesmo nome\n",
    "client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "# Criar tabela no CH\n",
    "client.command(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "        {schema}\n",
    "        ) ENGINE = MergeTree\n",
    "        ORDER BY (DATA)\n",
    "''')\n",
    "\n",
    "\n",
    "# Exportar os dados para o clickhouse\n",
    "client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
