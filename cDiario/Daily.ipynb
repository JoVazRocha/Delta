{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo do código\n",
    "\n",
    "### <u>Código que gera os ficheiros para estudo do comportamento da Delta</u>\n",
    "---\n",
    "O objectivo é receber dados da Delta e devolver um conjunto de métricas para prever roturas. Devolve um ficheiro que pode entrar no código 1 para juntar aos dados dos ninjas.\n",
    "\n",
    "---\n",
    "- Inputs\n",
    "\n",
    "> __Dados completos da Delta em pastas de ficheiros__ (de azul a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "                    ou\n",
    "\n",
    "> __Ficheiro já completo__ (de vermelho a verde)\n",
    "> - Stocks e trânsito, Sellout do dia anterior\n",
    "\n",
    "- Outputs\n",
    "\n",
    "> __Ficheiro com produtos em causa__ em formato Long\n",
    "\n",
    "> __Métricas novas:__\n",
    "> - Roturas de Stock e Pré-rotura\n",
    "> - Sinal\n",
    "> - Ciclos e Adequação de Stock\n",
    "> - MSA (média de sellouts 10 dias antes)\n",
    "> - STK (Stock disponível + trânsito)\n",
    "> - (Novo) Balanço médio, mediano, liberal e conservador \n",
    "> - (Novo) Dias para a rotura de stock e de prateleira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 359 ms\n",
      "Wall time: 368 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "def escrever_csv(dfa, nome): \n",
    "    dfa.to_csv(f'D:\\\\B&N Dados\\\\Delta\\\\Stocks\\\\StocksTotal\\\\{nome}.csv', index=False)\n",
    "\n",
    "def escrever_txt(dfa, nome): \n",
    "    dfa.to_csv(f'D:\\\\B&N Dados\\\\Delta\\\\Stocks\\\\StocksTotal\\\\{nome}.txt', index=False, header=False)\n",
    "    \n",
    "def escrever_excel(dfa, nome):\n",
    "    dfa.to_excel(f'D:\\\\B&N Dados\\\\Delta\\\\Stocks\\\\StocksTotal\\\\{nome}.xlsx' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:red\"><u>Ler Ficheiro Completo</u> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.2 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Ler os ficheiros\n",
    "df_2022 = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Stocks\\\\Stocks2022\\\\Stocks_Delta_2022_Limpo.csv')\n",
    "df_2023 = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Stocks\\\\Stocks2023\\\\Stocks_Delta_2023_Limpo.csv')\n",
    "\n",
    "# Juntar as bases\n",
    "dataframes = [df_2022, df_2023]\n",
    "df_Fusão = pd.concat(dataframes, ignore_index=True)\n",
    "df_Fusão['DATA']= pd.to_datetime(df_Fusão['DATA'], format='%Y-%m-%d')\n",
    "df_Fusão = df_Fusão.rename(columns={\"INTRANSIT\": \"INTRANSIT_1_Dias_Antes\", \n",
    "                                    \"EXPECTED\": \"EXPECTED_1_Dias_Antes\", \n",
    "                                    \"PRES_STOCK\": \"PRES_STOCK_1_Dias_Antes\"})\n",
    "\n",
    "\n",
    "# Ficheiro de previsão\n",
    "df_Prophet = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Forecast\\\\Prophet.csv')\n",
    "df_Prophet['DATA']= pd.to_datetime(df_Prophet['DATA'], format='%Y-%m-%d')\n",
    "\n",
    "df_XGBoost = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Forecast\\\\XGBoost.csv')\n",
    "df_XGBoost['DATA']= pd.to_datetime(df_XGBoost['DATA'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Opcional:</font> Definir produtos em causa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produtos específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 433 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ler ficheiro dos produtos e lojas para dataframe\n",
    "df_produtos = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Piloto\\\\produtos.txt', header=None)\n",
    "df_lojas = pd.read_csv('D:\\\\B&N Dados\\\\Delta\\\\Piloto\\\\lojas.txt', header=None)\n",
    "\n",
    "# Passar para uma lista\n",
    "produtos = df_produtos[0].tolist()\n",
    "#lojas = df_lojas[0].tolist()\n",
    "produtos+=[\"CAFÉ DELTA Q MYTHIQ 10CAP\", \"CAFÉ DELTA Q MYTHIQ XL 40CAP\", \"CAFÉ DELTA Q QALIDUS 10CAP\", \"CAFÉ DELTA Q QALIDUS 40CAP\",\n",
    "                'CAFÉ DELTA Q QHARACTER 10CAP','CAFÉ DELTA Q QHARACTER 40CAP','CAFÉ DELTA Q DEQAFEINATUS 10CAP','CAFÉ DELTA Q DEQAFEINATUS XL 40CAP',\n",
    "               'CAFÉ DELTA MOAGEM UNIVERSAL ANGOLA 220G','CAFÉ DELTA MOAGEM UNIVERSAL BRASIL 220G']\n",
    "# Alterar o dataframe para apenas incluir os produtos e lojas em causa\n",
    "#dfFinal = df_Fusão[(df_Fusão[\"DESC_ARTIGO\"].isin(produtos)) & (df_Fusão[\"STORE_NAME\"].isin(lojas))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Fim</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = df_Fusão[(df_Fusão[\"DESC_ARTIGO\"].isin(produtos))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['SELLOUT'] = dfFinal.groupby([\"STORE\",\"EAN\"])['SELLOUT_1_Dias_Antes'].shift(-1)\n",
    "dfFinal['STOCK'] = dfFinal.groupby([\"STORE\",\"EAN\"])['STOCK_1_Dias_Antes'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>EAN</th>\n",
       "      <th>DESC_ARTIGO</th>\n",
       "      <th>STORE</th>\n",
       "      <th>STORE_NAME</th>\n",
       "      <th>INTRANSIT</th>\n",
       "      <th>EXPECTED</th>\n",
       "      <th>PRES_STOCK</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>STOCK_1_Dias_Antes</th>\n",
       "      <th>SELLOUT</th>\n",
       "      <th>SELLOUT_1_Dias_Antes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60608</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5609060007087</td>\n",
       "      <td>BEBIDA CEREAIS DELTA C/20%CAFE FR 200G</td>\n",
       "      <td>1</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>120</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60609</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>5609060007087</td>\n",
       "      <td>BEBIDA CEREAIS DELTA C/20%CAFE FR 200G</td>\n",
       "      <td>1</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>120</td>\n",
       "      <td>137.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60610</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>5609060007087</td>\n",
       "      <td>BEBIDA CEREAIS DELTA C/20%CAFE FR 200G</td>\n",
       "      <td>1</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>175.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60611</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>5609060007087</td>\n",
       "      <td>BEBIDA CEREAIS DELTA C/20%CAFE FR 200G</td>\n",
       "      <td>1</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>162.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60612</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>5609060007087</td>\n",
       "      <td>BEBIDA CEREAIS DELTA C/20%CAFE FR 200G</td>\n",
       "      <td>1</td>\n",
       "      <td>CNT MATOSINHOS</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>120</td>\n",
       "      <td>195.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DATA            EAN                             DESC_ARTIGO  \\\n",
       "60608 2022-01-01  5609060007087  BEBIDA CEREAIS DELTA C/20%CAFE FR 200G   \n",
       "60609 2022-01-02  5609060007087  BEBIDA CEREAIS DELTA C/20%CAFE FR 200G   \n",
       "60610 2022-01-03  5609060007087  BEBIDA CEREAIS DELTA C/20%CAFE FR 200G   \n",
       "60611 2022-01-04  5609060007087  BEBIDA CEREAIS DELTA C/20%CAFE FR 200G   \n",
       "60612 2022-01-05  5609060007087  BEBIDA CEREAIS DELTA C/20%CAFE FR 200G   \n",
       "\n",
       "       STORE      STORE_NAME  INTRANSIT  EXPECTED  PRES_STOCK  STOCK  \\\n",
       "60608      1  CNT MATOSINHOS          0        48         120  151.0   \n",
       "60609      1  CNT MATOSINHOS          0        48         120  137.0   \n",
       "60610      1  CNT MATOSINHOS         48         0         120  175.0   \n",
       "60611      1  CNT MATOSINHOS          0         0         120  162.0   \n",
       "60612      1  CNT MATOSINHOS          0        48         120  195.0   \n",
       "\n",
       "       STOCK_1_Dias_Antes  SELLOUT  SELLOUT_1_Dias_Antes  \n",
       "60608               151.0      0.0                  11.0  \n",
       "60609               151.0     14.0                   0.0  \n",
       "60610               137.0     10.0                  14.0  \n",
       "60611               175.0     13.0                  10.0  \n",
       "60612               162.0     15.0                  13.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fundir o Ficheiro da Delta com a previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.42 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prophet\n",
    "dfFinal = pd.merge(dfFinal, df_Prophet[['DATA', 'STORE', 'DESC_ARTIGO', 'Prophet']], how=\"left\", on=['DATA', 'STORE', 'DESC_ARTIGO',] )\n",
    "# XGBoost\n",
    "dfFinal = pd.merge(dfFinal, df_XGBoost[['DATA', 'STORE', 'DESC_ARTIGO', 'XGBoost']], how=\"left\", on=['DATA', 'STORE', 'DESC_ARTIGO',] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>Ficheiro Lido<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def historico(titulo, alvo, function):\n",
    "    dfFinal[\"%s_30\" %titulo] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['%s' %alvo].shift(1).transform(lambda x: x.rolling(window=30).apply(function))\n",
    "    dfFinal[\"%s_60\" %titulo] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['%s' %alvo].shift(1).transform(lambda x: x.rolling(window=60).apply(function))\n",
    "    dfFinal[\"%s_120\" %titulo] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['%s' %alvo].shift(1).transform(lambda x: x.rolling(window=120).apply(function))\n",
    "    dfFinal[\"%s_180\" %titulo] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['%s' %alvo].shift(1).transform(lambda x: x.rolling(window=160).apply(function))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas interessantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ROTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir coluna de rotura (se stock menor ou igual a 0 e existe Linear)\n",
    "\n",
    "dfFinal[\"ROTURA\"] = np.where((dfFinal[\"STOCK\"] <= 0) & (dfFinal[\"PRES_STOCK\"] > 0), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - PRÉ_ROTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir coluna de rotura (se stock menor ou igual a 0)\n",
    "\n",
    "dfFinal[\"PRE_ROTURA\"] = (dfFinal[\"STOCK\"] < dfFinal[\"PRES_STOCK\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colunas de métricas 30, 60, 120 e 180 dias antes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Volatilidade de Procura: <br>\n",
    "coeficiente de variação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.53 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfFinal[\"Volatilidade_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).std())/\n",
    "                             dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Volatilidade_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).std())/\n",
    "                             dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Volatilidade_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).std())/\n",
    "                              dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Volatilidade_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).std())/\n",
    "                              dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de Rotura: <br>\n",
    "média de roturas $* 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.19 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#historico(\"Percentagem_Roturas\", \"ROTURA\", pd.Series.mean)\n",
    "\n",
    "dfFinal[\"Percentagem_Roturas_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Percentagem_Roturas_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Percentagem_Roturas_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Percentagem_Roturas_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de Supply:<br>\n",
    "média de vezes que foi pedido stock $*100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"New_Supply\"] = np.where((dfFinal[\"EXPECTED\"].shift(1)==0) & (dfFinal[\"EXPECTED\"]>0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historico(\"Percentagem_Supply\", \"New_Supply\", pd.Series.mean)\n",
    "\n",
    "dfFinal[\"Percentagem_Supply_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).sum())/30)*100\n",
    "\n",
    "dfFinal[\"Percentagem_Supply_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).sum())/60)*100\n",
    "\n",
    "dfFinal[\"Percentagem_Supply_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).sum())/120)*100\n",
    "\n",
    "dfFinal[\"Percentagem_Supply_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['New_Supply'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).sum())/180)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Efeito fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['SELLOUT_fds'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([4,5,6])][\"SELLOUT\"].copy()\n",
    "dfFinal['SELLOUT_semana'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([0,1,2,3])][\"SELLOUT\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Efeito_Fds_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).median())/\n",
    "                            (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).median())))-1\n",
    "\n",
    "dfFinal[\"Efeito_Fds_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).median())/\n",
    "                            (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).median())))-1\n",
    "\n",
    "dfFinal[\"Efeito_Fds_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).median())/\n",
    "                            (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).median())))-1\n",
    "\n",
    "dfFinal[\"Efeito_Fds_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).median())/\n",
    "                            (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).median())))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo médio inter-supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"InterSupply\"] = np.where(dfFinal[\"EXPECTED\"]==0, 1, 0)\n",
    "\n",
    "groups = (dfFinal['InterSupply'] != dfFinal['InterSupply'].shift()).cumsum()\n",
    "result = dfFinal.groupby(groups).agg({'DATA': 'first', 'DESC_ARTIGO': 'first', 'STORE': 'first', 'InterSupply': 'sum'}).reset_index(drop=True)\n",
    "result = result[result['InterSupply'] > 0]\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=['InterSupply'])\n",
    "\n",
    "dfFinal = pd.merge(dfFinal, result, how=\"left\", on=[\"DATA\",\"DESC_ARTIGO\", \"STORE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historico(\"InterSupplyMed\", \"InterSupply\", pd.Series.mean)\n",
    "\n",
    "dfFinal[\"InterSupplyMed_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=30, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"InterSupplyMed_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=60, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"InterSupplyMed_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=120, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"InterSupplyMed_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['InterSupply'].transform(lambda x: x.rolling(window=180, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tempo indisponível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''O que vai acontecer é: no primeiro dia em que há rotura vai aparecer a soma de todos os dias com rotura a seguir a esse!\n",
    "Todos os outros valores serão NaN para não serem considerados quando for feita a média. Assim a média corresponderá ao\n",
    "número médio de dias em que se deixa um produto em rotura.'''\n",
    "\n",
    "dfFinal[\"Tempo_Indisponível\"] = np.where(dfFinal[\"ROTURA\"]==1, 1, 0)\n",
    "\n",
    "groups = (dfFinal['Tempo_Indisponível'] != dfFinal['Tempo_Indisponível'].shift()).cumsum()\n",
    "result = dfFinal.groupby(groups).agg({'DATA': 'first', 'DESC_ARTIGO': 'first', 'STORE': 'first', 'Tempo_Indisponível': 'sum'}).reset_index(drop=True)\n",
    "result = result[result['Tempo_Indisponível'] > 0]\n",
    "\n",
    "dfFinal = dfFinal.drop(columns=['Tempo_Indisponível'])\n",
    "\n",
    "dfFinal = pd.merge(dfFinal, result, how=\"left\", on=[\"DATA\",\"DESC_ARTIGO\", \"STORE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historico(\"Percentagem_Supply\", \"New_Supply\", pd.Series.mean)\n",
    "\n",
    "dfFinal[\"Tempo_Indisponível_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Tempo_Indisponível'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Tempo_Indisponível_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Tempo_Indisponível'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Tempo_Indisponível_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Tempo_Indisponível'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))\n",
    "\n",
    "dfFinal[\"Tempo_Indisponível_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Tempo_Indisponível'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias em Stock Borderline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Stock_Borderline\"] = np.where(dfFinal[\"STOCK\"]<0.2*dfFinal[\"PRES_STOCK\"], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Tempo_Stock_Borderline_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Stock_Borderline'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Stock_Borderline_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Stock_Borderline'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Stock_Borderline_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Stock_Borderline'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Stock_Borderline_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Stock_Borderline'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias de Linear Incompleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Linear_Incompleto\"] = np.where(dfFinal[\"STOCK\"]<dfFinal[\"PRES_STOCK\"], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Tempo_Linear_Incompleto_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Linear_Incompleto'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Linear_Incompleto_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Linear_Incompleto'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Linear_Incompleto_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Linear_Incompleto'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Tempo_Linear_Incompleto_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Linear_Incompleto'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentagem de dias sem vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal[\"Sem_Vendas\"] = np.where(dfFinal[\"SELLOUT\"] == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historico(\"Sem_Vendas\", \"Sem_Vendas\", pd.Series.mean)\n",
    "\n",
    "dfFinal[\"Sem_Vendas_30\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Sem_Vendas_60\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Sem_Vendas_120\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).mean()))*100\n",
    "\n",
    "dfFinal[\"Sem_Vendas_180\"] = (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['Sem_Vendas'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).mean()))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Vendas perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal['ROTURA_fds'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([4,5,6])][\"ROTURA\"].copy()\n",
    "dfFinal['ROTURA_semana'] = dfFinal[dfFinal['DATA'].dt.weekday.isin([0,1,2,3])][\"ROTURA\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Mediana de fins de semana e mediana de semana a multiplicar pelo nº de dias em que há rotura, soma dos valores para\n",
    "ter as perdas de vendas estimadas'''\n",
    "\n",
    "#mediana fds*roturas fds + mediana semana*roturas semana\n",
    "\n",
    "dfFinal[\"Vendas_Perdidas_30\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).median())) + \n",
    "                                (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=30, min_periods=1).median()))\n",
    ")\n",
    "dfFinal[\"Vendas_Perdidas_60\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).median())) + \n",
    "                                (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=60, min_periods=1).median()))\n",
    ")\n",
    "dfFinal[\"Vendas_Perdidas_120\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).median())) + \n",
    "                                 (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=120, min_periods=1).median()))\n",
    ")\n",
    "dfFinal[\"Vendas_Perdidas_180\"] = ((dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_fds'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_fds'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).median())) + \n",
    "                                 (dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['ROTURA_semana'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).sum()) * dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT_semana'].shift(1).transform(lambda x: x.rolling(window=180, min_periods=1).median()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas até 10 dias antes:\n",
    "\n",
    "- INSTRANSIT\n",
    "- EXPECTED\n",
    "- SELLOUT\n",
    "- CICLOS\n",
    "- Dias para Rotura\n",
    "- Adequação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos dias antes:\n",
    "\n",
    "diaI=4         #dia inicial\n",
    "diaF=5       #dia final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para colunas de dias anteriores\n",
    "def dias(df, dia, coluna):         #dia é quantos dias antes\n",
    "    a=int(dia)\n",
    "\n",
    "    valores = df.groupby(['DESC_ARTIGO', 'STORE'])[coluna].transform(lambda x: x.shift(a))\n",
    "    valores[:a] = np.nan\n",
    "    \n",
    "    df.loc[:,'%s_%s_Dias_Antes' % (coluna, a)] = valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - SELLOUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.66 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Usar função para sellouts até 10 dias antes\n",
    "\n",
    "for i in range(diaI+1, diaF+1):\n",
    "    dias(dfFinal, i, \"SELLOUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar função para Stocks até 10 dias antes\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"STOCK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > - Ordenar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "previsões = 2\n",
    "dfOrg = dfFinal.iloc[:, np.r_[:9, 10, 12, 13, 12+previsões, 13+previsões, 9, \n",
    "                                (23+previsões):(32+previsões), 11, (14+previsões):(23+previsões)]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_columns = dfOrg.columns.tolist()\n",
    "indexed_columns = [f\"{index}: {column}\" for index, column in enumerate(df_columns)]\n",
    "indexed_columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfFinal = dfFinal.iloc[:, np.r_[:9, 10, 12, 13, 12+previsões, 13+previsões, 9, \n",
    "                                (23+previsões):(32+previsões), 11, (14+previsões):(23+previsões)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - INTRANSIT e EXPECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar função para Trânsito até 10 dias antes\n",
    "\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"INTRANSIT\")\n",
    "    \n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"EXPECTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - STK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STK do dia = soma dos stocks em loja com os stocks em trânsito no próprio dia\n",
    "\n",
    "dfFinal[\"STK\"] = dfFinal[\"STOCK\"] + dfFinal[\"INTRANSIT\"] + dfFinal[\"EXPECTED\"]\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"STK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA do dia = média dos sellouts dos 10 dias anteriores ao dia em causa\n",
    "\n",
    "dfFinal[\"MSA10\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "dfFinal[\"MSA10Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=10, min_periods=1).std())\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"MSA10\")\n",
    "\n",
    "    \n",
    "dfFinal[\"MSA20\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n",
    "dfFinal[\"MSA20Dp\"] = dfFinal.groupby(['DESC_ARTIGO', \"STORE\"])['SELLOUT'].shift(1).transform(lambda x: x.rolling(window=20, min_periods=1).std())\n",
    "  \n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"MSA20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - CICLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluna de Ciclos de reposição\n",
    "\n",
    "dfFinal[\"CICLOS\"] = dfFinal[\"STOCK\"]/dfFinal[\"PRES_STOCK\"]\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"CICLOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfFinal = dfFinal.copy()\n",
    "# Dias para a rotura mas com o Sellout médio (móvel) dos últimos 10 dias \n",
    "dfFinal[\"Dias_para_Rotura_Stock\"] = dfFinal[\"STOCK\"] / dfFinal[\"MSA10\"]\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"Dias_para_Rotura_Stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dias para rotura de Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a métrica: Preslinear / med(Sellouts 10 dias)\n",
    "dfFinal['Dias_Duração_Linear'] = dfFinal[\"PRES_STOCK\"] / dfFinal[\"MSA10\"]\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"Dias_Duração_Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Adequação de Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluna de adequação de stock\n",
    "\n",
    "\n",
    "dfFinal[\"Adequação\"]= np.where(dfFinal[\"CICLOS\"] > 1.1, \"Stock Suficiente\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]>=dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Adequado\", \n",
    "                      np.where((dfFinal[\"CICLOS\"] <= 1.1) & (dfFinal[\"INTRANSIT\"]+dfFinal[\"EXPECTED\"]+dfFinal[\"STOCK\"]<dfFinal[\"PRES_STOCK\"]), \"Stock Insuf c Forn Desadequado\", \n",
    "                      \"\")))\n",
    "\n",
    "for i in range(diaI, diaF+1):\n",
    "    dias(dfFinal, i, \"Adequação\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dfFinal[[\"DATA\",\"STORE\",\"DESC_ARTIGO\",\"STOCK\",\"INTRANSIT\",\"EXPECTED\",\"SELLOUT\",\"Balance\",\"Balance_4_Dias_Antes\",\"Balance_Liberal\",\"Balance_Liberal_4_Dias_Antes\",\"Balance_Mediano_4_Dias_Antes\",\"Balance_Conservador_4_Dias_Antes\"]].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escrever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0's e 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo dataframe que apenas inclui dias em que existe a 1ª rotura e o dia anterior a essa rotura\n",
    "\n",
    "# Tirar 1's depois do primeiro\n",
    "dfFinalLimitado = dfFinal[~((dfFinal[\"ROTURA\"] == 1) & (dfFinal[\"ROTURA\"].shift(1) == 1))].copy()\n",
    "\n",
    "# Apenas incluir primeira rotura\n",
    "dfFinalLimitadoRoturas = dfFinal[((dfFinal[\"ROTURA\"] == 1) & (dfFinal[\"ROTURA\"].shift(1) == 0))].copy() #| ((dfFinal[\"ROTURA\"] == 0) & (dfFinal[\"ROTURA\"].shift(-1) == 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dias certos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal2 = dfFinal.loc[(dfFinal['DATA'] >= '2023-01-01') ].copy()\n",
    "#df_DiasCertos = dfFinalLimitado.loc[(dfFinalLimitado['DATA'] >= '2023-01-01') ].copy()\n",
    "#df_RoturasDiasCertos = dfFinalLimitadoRoturas.loc[(dfFinalLimitadoRoturas['DATA'] >= '2023-01-01') ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-01-01T00:00:00.000000000', '2023-01-02T00:00:00.000000000',\n",
       "       '2023-01-03T00:00:00.000000000', '2023-01-04T00:00:00.000000000',\n",
       "       '2023-01-05T00:00:00.000000000', '2023-01-06T00:00:00.000000000',\n",
       "       '2023-01-07T00:00:00.000000000', '2023-01-08T00:00:00.000000000',\n",
       "       '2023-01-09T00:00:00.000000000', '2023-01-10T00:00:00.000000000',\n",
       "       '2023-01-11T00:00:00.000000000', '2023-01-12T00:00:00.000000000',\n",
       "       '2023-01-13T00:00:00.000000000', '2023-01-14T00:00:00.000000000',\n",
       "       '2023-01-15T00:00:00.000000000', '2023-01-16T00:00:00.000000000',\n",
       "       '2023-01-17T00:00:00.000000000', '2023-01-18T00:00:00.000000000',\n",
       "       '2023-01-19T00:00:00.000000000', '2023-01-20T00:00:00.000000000',\n",
       "       '2023-01-21T00:00:00.000000000', '2023-01-22T00:00:00.000000000',\n",
       "       '2023-01-23T00:00:00.000000000', '2023-01-24T00:00:00.000000000',\n",
       "       '2023-01-25T00:00:00.000000000', '2023-01-26T00:00:00.000000000',\n",
       "       '2023-01-27T00:00:00.000000000', '2023-01-28T00:00:00.000000000',\n",
       "       '2023-01-29T00:00:00.000000000', '2023-01-30T00:00:00.000000000',\n",
       "       '2023-01-31T00:00:00.000000000', '2023-02-01T00:00:00.000000000',\n",
       "       '2023-02-02T00:00:00.000000000', '2023-02-03T00:00:00.000000000',\n",
       "       '2023-02-04T00:00:00.000000000', '2023-02-05T00:00:00.000000000',\n",
       "       '2023-02-07T00:00:00.000000000', '2023-02-08T00:00:00.000000000',\n",
       "       '2023-02-09T00:00:00.000000000', '2023-02-10T00:00:00.000000000',\n",
       "       '2023-02-11T00:00:00.000000000', '2023-02-12T00:00:00.000000000',\n",
       "       '2023-02-13T00:00:00.000000000', '2023-02-14T00:00:00.000000000',\n",
       "       '2023-02-15T00:00:00.000000000', '2023-02-16T00:00:00.000000000',\n",
       "       '2023-02-17T00:00:00.000000000', '2023-02-18T00:00:00.000000000',\n",
       "       '2023-02-19T00:00:00.000000000', '2023-02-20T00:00:00.000000000',\n",
       "       '2023-02-21T00:00:00.000000000', '2023-02-22T00:00:00.000000000',\n",
       "       '2023-02-24T00:00:00.000000000', '2023-02-25T00:00:00.000000000',\n",
       "       '2023-02-26T00:00:00.000000000', '2023-02-27T00:00:00.000000000',\n",
       "       '2023-02-28T00:00:00.000000000', '2023-03-01T00:00:00.000000000',\n",
       "       '2023-03-02T00:00:00.000000000', '2023-03-03T00:00:00.000000000',\n",
       "       '2023-03-04T00:00:00.000000000', '2023-03-05T00:00:00.000000000',\n",
       "       '2023-03-06T00:00:00.000000000', '2023-03-07T00:00:00.000000000',\n",
       "       '2023-03-08T00:00:00.000000000', '2023-03-09T00:00:00.000000000',\n",
       "       '2023-03-10T00:00:00.000000000', '2023-03-11T00:00:00.000000000',\n",
       "       '2023-03-12T00:00:00.000000000', '2023-03-13T00:00:00.000000000',\n",
       "       '2023-03-14T00:00:00.000000000', '2023-03-15T00:00:00.000000000',\n",
       "       '2023-03-16T00:00:00.000000000', '2023-03-17T00:00:00.000000000',\n",
       "       '2023-03-18T00:00:00.000000000', '2023-03-19T00:00:00.000000000',\n",
       "       '2023-03-20T00:00:00.000000000', '2023-03-21T00:00:00.000000000',\n",
       "       '2023-03-22T00:00:00.000000000', '2023-03-23T00:00:00.000000000',\n",
       "       '2023-03-24T00:00:00.000000000', '2023-03-25T00:00:00.000000000',\n",
       "       '2023-03-26T00:00:00.000000000', '2023-03-27T00:00:00.000000000',\n",
       "       '2023-03-28T00:00:00.000000000', '2023-03-29T00:00:00.000000000',\n",
       "       '2023-03-30T00:00:00.000000000', '2023-03-31T00:00:00.000000000',\n",
       "       '2023-04-01T00:00:00.000000000', '2023-04-02T00:00:00.000000000',\n",
       "       '2023-04-03T00:00:00.000000000', '2023-04-04T00:00:00.000000000',\n",
       "       '2023-04-05T00:00:00.000000000', '2023-04-06T00:00:00.000000000',\n",
       "       '2023-04-07T00:00:00.000000000', '2023-04-08T00:00:00.000000000',\n",
       "       '2023-04-09T00:00:00.000000000', '2023-04-10T00:00:00.000000000',\n",
       "       '2023-04-11T00:00:00.000000000', '2023-04-12T00:00:00.000000000',\n",
       "       '2023-04-13T00:00:00.000000000', '2023-04-14T00:00:00.000000000',\n",
       "       '2023-04-15T00:00:00.000000000', '2023-04-16T00:00:00.000000000',\n",
       "       '2023-04-17T00:00:00.000000000', '2023-04-18T00:00:00.000000000',\n",
       "       '2023-04-19T00:00:00.000000000', '2023-04-20T00:00:00.000000000',\n",
       "       '2023-04-21T00:00:00.000000000', '2023-04-22T00:00:00.000000000',\n",
       "       '2023-04-23T00:00:00.000000000', '2023-04-24T00:00:00.000000000',\n",
       "       '2023-04-25T00:00:00.000000000', '2023-04-26T00:00:00.000000000',\n",
       "       '2023-04-27T00:00:00.000000000', '2023-04-28T00:00:00.000000000',\n",
       "       '2023-04-29T00:00:00.000000000', '2023-04-30T00:00:00.000000000',\n",
       "       '2023-05-01T00:00:00.000000000', '2023-05-02T00:00:00.000000000',\n",
       "       '2023-05-03T00:00:00.000000000', '2023-05-04T00:00:00.000000000',\n",
       "       '2023-05-05T00:00:00.000000000', '2023-05-06T00:00:00.000000000',\n",
       "       '2023-05-07T00:00:00.000000000', '2023-05-08T00:00:00.000000000',\n",
       "       '2023-05-09T00:00:00.000000000', '2023-05-10T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinal2.DATA.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passar para csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 524 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "escrever_csv(filtered_oi, \"Métricas3060120_Piloto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "escrever_csv(dfFinal2, \"Mil_MétricasCerto07\")\n",
    "#escrever_csv(df_DiasCertos, \"StocksDelta_2023_10prod_Limpo\")\n",
    "#escrever_csv(df_RoturasDiasCertos, \"StocksDelta_2023_10prod_Roturas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escrever_excel(dataFrame, nomeFicheiro):\n",
    "    dataFrame.to_excel('D:\\\\B&N Dados\\\\Delta\\\\Piloto\\\\%s.xlsx' %nomeFicheiro, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "escrever_excel(oi, \"ParaTestarCoisas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m oiEscrever \u001b[38;5;241m=\u001b[39m oi[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-14\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39moi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-16\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-21\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39moi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-23\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-28\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39moi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-04-30\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-05-05\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39moi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-05-07\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3_recent\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "oiEscrever = oi[(\"2023-04-14\"<=oi[\"DATA\"]<=\"2023-04-16\") or (\"2023-04-21\"<=oi[\"DATA\"]<=\"2023-04-23\") or (\"2023-04-28\"<=oi[\"DATA\"]<=\"2023-04-30\") or (\"2023-05-05\"<=oi[\"DATA\"]<=\"2023-05-07\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_oi = oi[oi[\"DATA\"].between(\"2023-04-14\", \"2023-04-16\") |\n",
    "                  oi[\"DATA\"].between(\"2023-04-21\", \"2023-04-23\") |\n",
    "                  oi[\"DATA\"].between(\"2023-04-28\", \"2023-04-30\") |\n",
    "                  oi[\"DATA\"].between(\"2023-05-05\", \"2023-05-07\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "from clickhouse_driver import Client\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = clickhouse_connect.get_client(host='ch.brandsandninjas.com', \n",
    "                                       port=443, \n",
    "                                       username='chninja', \n",
    "                                       password='ku43ueqnB5Q0AYb2C4FsJRTc7qX',\n",
    "                                       database = \"Delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualbase = 1\n",
    "\n",
    "if qualbase == 1:\n",
    "    baseCH = dfFinal.copy()\n",
    "    tabela = \"Daily\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Algumas alterações\n",
    "baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar a Data para datetime\n",
    "\n",
    "baseCH.columns = [unidecode(col) for col in baseCH.columns]        # Tirar acentos e afins dos nomes das colunas porque \n",
    "                                                                   # o Clickhouse não gosta\n",
    "\n",
    "\n",
    "## Listas para definir os tipos de dados de cada coluna a inserir\n",
    "\n",
    "data = [\"DATA\"]\n",
    "#data = [col for col in baseCH.columns if baseCH[col].dtype == 'datetime64[ns]']\n",
    "texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object']\n",
    "inteiros = [col for col in baseCH.columns if baseCH[col].dtype == 'int64' or baseCH[col].dtype == 'int32']\n",
    "floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "## Mudar inteiros para floats porque senão não pode haver missing values\n",
    "for col_name in inteiros:\n",
    "    baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "\n",
    "## Missing values em strings também estragam tudo\n",
    "baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "baseCH[texto] = baseCH[texto].astype(str)\n",
    "\n",
    "#Função que vai fazer a schema\n",
    "def schema(lista, tipo):\n",
    "    result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "    return result_list\n",
    "\n",
    "# Schema a ser feito\n",
    "data1 = schema(data, \"Date\")\n",
    "texto1 = schema(texto, \"String\")\n",
    "inteiros1 = schema(inteiros, \"Float64\")\n",
    "floats1 = schema(floats, \"Float64\")\n",
    "total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "# Split the input string by commas\n",
    "parts = schema.split(', ')\n",
    "# Process each part and wrap the first word in double quotes\n",
    "output_parts = []\n",
    "for part in parts:\n",
    "    words = part.split()\n",
    "    if words:\n",
    "        first_word = words[0]\n",
    "        remaining_words = ' '.join(words[1:])\n",
    "        output_part = f'\"{first_word}\" {remaining_words}'\n",
    "        output_parts.append(output_part)\n",
    "# Join the modified parts back into a string\n",
    "schema = ', '.join(output_parts)\n",
    "\n",
    "# Eliminar tabela que possa existir no CH com o mesmo nome\n",
    "client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "# Criar tabela no CH\n",
    "client.command(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "        {schema}\n",
    "        ) ENGINE = MergeTree\n",
    "        ORDER BY (DATA)\n",
    "''')\n",
    "\n",
    "\n",
    "# Exportar os dados para o clickhouse\n",
    "client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "client = clickhouse_connect.get_client(host='e28fluocjc.europe-west4.gcp.clickhouse.cloud', \n",
    "                                       port=8443, \n",
    "                                       username='default', \n",
    "                                       password='N_Mx30OFTC1hN',\n",
    "                                       database='Delta')\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "baseCH['DATA']= pd.to_datetime(baseCH['DATA'], format='%Y-%m-%d')  # Passar para datetime\n",
    "new_columns = [unidecode(col) for col in baseCH.columns]           # Tirar acentos e afins\n",
    "baseCH.columns = new_columns                                       # Aplicar alterações da linha anterior\n",
    "###\n",
    "\n",
    "# Tipos de dados\n",
    "data = [\"DATA\"]\n",
    "texto = [col for col in baseCH.columns if baseCH[col].dtype == 'object']\n",
    "inteiros = [col for col in baseCH.columns if baseCH[col].dtype in ['int64', 'int32']]\n",
    "floats = [col for col in baseCH.columns if baseCH[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "# Só floats é que permitem missing values\n",
    "for col_name in inteiros:\n",
    "    baseCH[col_name] = baseCH[col_name].astype(float)\n",
    "# Missing values em strings estragam tudo\n",
    "baseCH[texto] = baseCH[texto].fillna(\"-\")\n",
    "\n",
    "\n",
    "def schema(lista, tipo):\n",
    "    result_list = [f\"{element} {tipo}\" for element in lista]\n",
    "    return result_list\n",
    "\n",
    "data1 = schema(data, \"Date\")\n",
    "texto1 = schema(texto, \"String\")\n",
    "inteiros1 = schema(inteiros, \"Float64\")\n",
    "floats1 = schema(floats, \"Float64\")\n",
    "total = tuple(data1 + texto1 + inteiros1 + floats1)\n",
    "\n",
    "schema = ', '.join([column.replace(\"'\", \"\") for column in total])\n",
    "\n",
    "# Split the input string by commas\n",
    "parts = schema.split(', ')\n",
    "# Process each part and wrap the first word in double quotes\n",
    "output_parts = []\n",
    "for part in parts:\n",
    "    words = part.split()\n",
    "    if words:\n",
    "        first_word = words[0]\n",
    "        remaining_words = ' '.join(words[1:])\n",
    "        output_part = f'\"{first_word}\" {remaining_words}'\n",
    "        output_parts.append(output_part)\n",
    "# Join the modified parts back into a string\n",
    "schema = ', '.join(output_parts)\n",
    "\n",
    "tabela = \"Daily\"\n",
    "\n",
    "# Eliminar tabela no CH\n",
    "client.command(f'DROP TABLE IF EXISTS {tabela}')\n",
    "\n",
    "# Criar tabela no CH\n",
    "client.command(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {tabela} (\n",
    "        {schema}\n",
    "        ) ENGINE = MergeTree\n",
    "        ORDER BY (DATA)\n",
    "''')\n",
    "\n",
    "client.insert_df(tabela, baseCH, column_names=baseCH.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
